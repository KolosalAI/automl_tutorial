{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3db47d",
   "metadata": {},
   "source": [
    "# üöÄ AutoML Platforms Comprehensive Comparison\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive benchmark comparison between multiple AutoML platforms including:\n",
    "- **Kolosal-AutoML** (Genta Technology)\n",
    "- **FLAML** (Microsoft)\n",
    "- **Auto-sklearn**\n",
    "- **TPOT** (Tree-based Pipeline Optimization Tool)\n",
    "- **H2O AutoML**\n",
    "- **AutoGluon** (Amazon)\n",
    "- **PyCaret**\n",
    "- **MLjar-Supervised**\n",
    "- **Standard ML Baseline**\n",
    "\n",
    "## Comparison Metrics\n",
    "- **Accuracy/Performance**: Model accuracy and other performance metrics\n",
    "- **Training Time**: Time required to train models\n",
    "- **Memory Usage**: Peak memory consumption during training\n",
    "- **Prediction Time**: Time to make predictions\n",
    "- **Model Interpretability**: Ease of understanding model decisions\n",
    "- **Ease of Use**: API simplicity and documentation quality\n",
    "\n",
    "## Datasets Used\n",
    "- Small datasets: Iris, Wine, Breast Cancer\n",
    "- Medium datasets: Digits, California Housing\n",
    "- Synthetic datasets for scalability testing\n",
    "\n",
    "Let's start the comprehensive comparison!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beda50f",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries\n",
    "\n",
    "First, let's import all necessary libraries for our AutoML comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6899b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Optimization modules not available: No module named 'modules'\n",
      "‚úÖ Kolosal AutoML available\n",
      "‚úÖ FLAML available\n",
      "‚ùå Auto-sklearn not available\n",
      "‚úÖ FLAML available\n",
      "‚ùå Auto-sklearn not available\n",
      "‚ùå TPOT not available\n",
      "‚ùå TPOT not available\n",
      "‚úÖ H2O AutoML available\n",
      "‚úÖ H2O AutoML available\n",
      "‚úÖ AutoGluon available\n",
      "‚ùå PyCaret not available\n",
      "‚úÖ MLjar-Supervised available\n",
      "\n",
      "üéØ Setup completed! Available frameworks: 6 (including Standard ML)\n",
      "‚úÖ AutoGluon available\n",
      "‚ùå PyCaret not available\n",
      "‚úÖ MLjar-Supervised available\n",
      "\n",
      "üéØ Setup completed! Available frameworks: 6 (including Standard ML)\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import (\n",
    "    load_iris, load_wine, load_breast_cancer, load_digits,\n",
    "    load_diabetes, make_classification, make_regression,\n",
    "    fetch_california_housing\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, classification_report\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Interactive widgets\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, interact_manual, fixed, IntProgress\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ipywidgets not available - some interactive features will be disabled\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "# Kolosal AutoML\n",
    "try:\n",
    "    from kolosal_automl.modules.configs import (\n",
    "        TaskType, OptimizationStrategy, MLTrainingEngineConfig,\n",
    "        PreprocessorConfig, NormalizationType\n",
    "    )\n",
    "    from kolosal_automl.modules.engine.train_engine import MLTrainingEngine\n",
    "    KOLOSAL_AVAILABLE = True\n",
    "    print(\"‚úÖ Kolosal AutoML available\")\n",
    "except ImportError as e:\n",
    "    KOLOSAL_AVAILABLE = False\n",
    "    print(f\"‚ùå Kolosal AutoML not available: {e}\")\n",
    "\n",
    "# FLAML\n",
    "try:\n",
    "    from flaml.automl.automl import AutoML as FLAML_AutoML\n",
    "    FLAML_AVAILABLE = True\n",
    "    print(\"‚úÖ FLAML available\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from flaml import AutoML as FLAML_AutoML\n",
    "        FLAML_AVAILABLE = True\n",
    "        print(\"‚úÖ FLAML available\")\n",
    "    except ImportError:\n",
    "        FLAML_AVAILABLE = False\n",
    "        print(\"‚ùå FLAML not available\")\n",
    "\n",
    "# Auto-sklearn\n",
    "try:\n",
    "    import autosklearn.classification\n",
    "    import autosklearn.regression\n",
    "    AUTOSKLEARN_AVAILABLE = True\n",
    "    print(\"‚úÖ Auto-sklearn available\")\n",
    "except ImportError:\n",
    "    AUTOSKLEARN_AVAILABLE = False\n",
    "    print(\"‚ùå Auto-sklearn not available\")\n",
    "\n",
    "# TPOT\n",
    "try:\n",
    "    from tpot import TPOTClassifier, TPOTRegressor\n",
    "    TPOT_AVAILABLE = True\n",
    "    print(\"‚úÖ TPOT available\")\n",
    "except ImportError:\n",
    "    TPOT_AVAILABLE = False\n",
    "    print(\"‚ùå TPOT not available\")\n",
    "\n",
    "# H2O AutoML\n",
    "try:\n",
    "    import h2o\n",
    "    from h2o.automl import H2OAutoML\n",
    "    H2O_AVAILABLE = True\n",
    "    print(\"‚úÖ H2O AutoML available\")\n",
    "except ImportError:\n",
    "    H2O_AVAILABLE = False\n",
    "    print(\"‚ùå H2O AutoML not available\")\n",
    "\n",
    "# AutoGluon\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    AUTOGLUON_AVAILABLE = True\n",
    "    print(\"‚úÖ AutoGluon available\")\n",
    "except ImportError:\n",
    "    AUTOGLUON_AVAILABLE = False\n",
    "    print(\"‚ùå AutoGluon not available\")\n",
    "\n",
    "# PyCaret\n",
    "try:\n",
    "    from pycaret.classification import setup as pycaret_setup_clf, compare_models, finalize_model\n",
    "    from pycaret.regression import setup as pycaret_setup_reg\n",
    "    PYCARET_AVAILABLE = True\n",
    "    print(\"‚úÖ PyCaret available\")\n",
    "except ImportError:\n",
    "    PYCARET_AVAILABLE = False\n",
    "    print(\"‚ùå PyCaret not available\")\n",
    "\n",
    "# MLjar-Supervised\n",
    "try:\n",
    "    from supervised.automl import AutoML as MLjarAutoML\n",
    "    MLJAR_AVAILABLE = True\n",
    "    print(\"‚úÖ MLjar-Supervised available\")\n",
    "except ImportError:\n",
    "    MLJAR_AVAILABLE = False\n",
    "    print(\"‚ùå MLjar-Supervised not available\")\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "available_frameworks_count = sum([\n",
    "    KOLOSAL_AVAILABLE, FLAML_AVAILABLE, AUTOSKLEARN_AVAILABLE, \n",
    "    TPOT_AVAILABLE, H2O_AVAILABLE, AUTOGLUON_AVAILABLE, \n",
    "    PYCARET_AVAILABLE, MLJAR_AVAILABLE\n",
    "]) + 1  # +1 for Standard ML\n",
    "\n",
    "print(f\"\\nüéØ Setup completed! Available frameworks: {available_frameworks_count} (including Standard ML)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d8d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Benchmark data structures initialized\n"
     ]
    }
   ],
   "source": [
    "# Data structure for benchmark results\n",
    "@dataclass\n",
    "class AutoMLBenchmarkResult:\n",
    "    \"\"\"Data class to store AutoML benchmark results.\"\"\"\n",
    "    experiment_id: str\n",
    "    approach: str\n",
    "    dataset_name: str\n",
    "    model_name: str\n",
    "    dataset_size: Tuple[int, int]\n",
    "    task_type: str\n",
    "    \n",
    "    # Performance metrics\n",
    "    training_time: float\n",
    "    prediction_time: float\n",
    "    memory_peak_mb: float\n",
    "    memory_final_mb: float\n",
    "    \n",
    "    # ML metrics\n",
    "    train_score: float\n",
    "    test_score: float\n",
    "    cv_score_mean: float\n",
    "    cv_score_std: float\n",
    "    \n",
    "    # Additional metrics\n",
    "    best_params: Dict[str, Any]\n",
    "    feature_count: int\n",
    "    model_size_mb: float\n",
    "    preprocessing_time: float\n",
    "    \n",
    "    # Error handling\n",
    "    success: bool\n",
    "    error_message: str = \"\"\n",
    "    framework_version: str = \"\"\n",
    "\n",
    "# Global variables for storing results\n",
    "benchmark_results = []\n",
    "experiment_id = f\"EXP_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(\"‚úÖ Benchmark data structures initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9028ada",
   "metadata": {},
   "source": [
    "# 2. Setup Datasets for Benchmarking\n",
    "\n",
    "We'll use a variety of datasets to test different aspects of each AutoML platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3871981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Available Datasets for Benchmarking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7eb1b_row0_col0, #T_7eb1b_row0_col1, #T_7eb1b_row0_col2, #T_7eb1b_row0_col3, #T_7eb1b_row0_col4, #T_7eb1b_row1_col0, #T_7eb1b_row1_col1, #T_7eb1b_row1_col2, #T_7eb1b_row1_col3, #T_7eb1b_row1_col4, #T_7eb1b_row2_col0, #T_7eb1b_row2_col1, #T_7eb1b_row2_col2, #T_7eb1b_row2_col3, #T_7eb1b_row2_col4, #T_7eb1b_row3_col0, #T_7eb1b_row3_col1, #T_7eb1b_row3_col2, #T_7eb1b_row3_col3, #T_7eb1b_row3_col4, #T_7eb1b_row4_col0, #T_7eb1b_row4_col1, #T_7eb1b_row4_col2, #T_7eb1b_row4_col3, #T_7eb1b_row4_col4, #T_7eb1b_row5_col0, #T_7eb1b_row5_col1, #T_7eb1b_row5_col2, #T_7eb1b_row5_col3, #T_7eb1b_row5_col4, #T_7eb1b_row6_col0, #T_7eb1b_row6_col1, #T_7eb1b_row6_col2, #T_7eb1b_row6_col3, #T_7eb1b_row6_col4, #T_7eb1b_row7_col0, #T_7eb1b_row7_col1, #T_7eb1b_row7_col2, #T_7eb1b_row7_col3, #T_7eb1b_row7_col4, #T_7eb1b_row8_col0, #T_7eb1b_row8_col1, #T_7eb1b_row8_col2, #T_7eb1b_row8_col3, #T_7eb1b_row8_col4, #T_7eb1b_row9_col0, #T_7eb1b_row9_col1, #T_7eb1b_row9_col2, #T_7eb1b_row9_col3, #T_7eb1b_row9_col4 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7eb1b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7eb1b_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
       "      <th id=\"T_7eb1b_level0_col1\" class=\"col_heading level0 col1\" >samples</th>\n",
       "      <th id=\"T_7eb1b_level0_col2\" class=\"col_heading level0 col2\" >features</th>\n",
       "      <th id=\"T_7eb1b_level0_col3\" class=\"col_heading level0 col3\" >type</th>\n",
       "      <th id=\"T_7eb1b_level0_col4\" class=\"col_heading level0 col4\" >category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7eb1b_row0_col0\" class=\"data row0 col0\" >iris</td>\n",
       "      <td id=\"T_7eb1b_row0_col1\" class=\"data row0 col1\" >150</td>\n",
       "      <td id=\"T_7eb1b_row0_col2\" class=\"data row0 col2\" >4</td>\n",
       "      <td id=\"T_7eb1b_row0_col3\" class=\"data row0 col3\" >classification</td>\n",
       "      <td id=\"T_7eb1b_row0_col4\" class=\"data row0 col4\" >small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7eb1b_row1_col0\" class=\"data row1 col0\" >wine</td>\n",
       "      <td id=\"T_7eb1b_row1_col1\" class=\"data row1 col1\" >178</td>\n",
       "      <td id=\"T_7eb1b_row1_col2\" class=\"data row1 col2\" >13</td>\n",
       "      <td id=\"T_7eb1b_row1_col3\" class=\"data row1 col3\" >classification</td>\n",
       "      <td id=\"T_7eb1b_row1_col4\" class=\"data row1 col4\" >small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7eb1b_row2_col0\" class=\"data row2 col0\" >breast_cancer</td>\n",
       "      <td id=\"T_7eb1b_row2_col1\" class=\"data row2 col1\" >569</td>\n",
       "      <td id=\"T_7eb1b_row2_col2\" class=\"data row2 col2\" >30</td>\n",
       "      <td id=\"T_7eb1b_row2_col3\" class=\"data row2 col3\" >classification</td>\n",
       "      <td id=\"T_7eb1b_row2_col4\" class=\"data row2 col4\" >small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7eb1b_row3_col0\" class=\"data row3 col0\" >digits</td>\n",
       "      <td id=\"T_7eb1b_row3_col1\" class=\"data row3 col1\" >1797</td>\n",
       "      <td id=\"T_7eb1b_row3_col2\" class=\"data row3 col2\" >64</td>\n",
       "      <td id=\"T_7eb1b_row3_col3\" class=\"data row3 col3\" >classification</td>\n",
       "      <td id=\"T_7eb1b_row3_col4\" class=\"data row3 col4\" >medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7eb1b_row4_col0\" class=\"data row4 col0\" >diabetes</td>\n",
       "      <td id=\"T_7eb1b_row4_col1\" class=\"data row4 col1\" >442</td>\n",
       "      <td id=\"T_7eb1b_row4_col2\" class=\"data row4 col2\" >10</td>\n",
       "      <td id=\"T_7eb1b_row4_col3\" class=\"data row4 col3\" >regression</td>\n",
       "      <td id=\"T_7eb1b_row4_col4\" class=\"data row4 col4\" >small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7eb1b_row5_col0\" class=\"data row5 col0\" >california_housing</td>\n",
       "      <td id=\"T_7eb1b_row5_col1\" class=\"data row5 col1\" >20640</td>\n",
       "      <td id=\"T_7eb1b_row5_col2\" class=\"data row5 col2\" >8</td>\n",
       "      <td id=\"T_7eb1b_row5_col3\" class=\"data row5 col3\" >regression</td>\n",
       "      <td id=\"T_7eb1b_row5_col4\" class=\"data row5 col4\" >large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7eb1b_row6_col0\" class=\"data row6 col0\" >synthetic_small_classification</td>\n",
       "      <td id=\"T_7eb1b_row6_col1\" class=\"data row6 col1\" >1000</td>\n",
       "      <td id=\"T_7eb1b_row6_col2\" class=\"data row6 col2\" >20</td>\n",
       "      <td id=\"T_7eb1b_row6_col3\" class=\"data row6 col3\" >classification</td>\n",
       "      <td id=\"T_7eb1b_row6_col4\" class=\"data row6 col4\" >small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7eb1b_row7_col0\" class=\"data row7 col0\" >synthetic_medium_classification</td>\n",
       "      <td id=\"T_7eb1b_row7_col1\" class=\"data row7 col1\" >5000</td>\n",
       "      <td id=\"T_7eb1b_row7_col2\" class=\"data row7 col2\" >50</td>\n",
       "      <td id=\"T_7eb1b_row7_col3\" class=\"data row7 col3\" >classification</td>\n",
       "      <td id=\"T_7eb1b_row7_col4\" class=\"data row7 col4\" >medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7eb1b_row8_col0\" class=\"data row8 col0\" >synthetic_small_regression</td>\n",
       "      <td id=\"T_7eb1b_row8_col1\" class=\"data row8 col1\" >1000</td>\n",
       "      <td id=\"T_7eb1b_row8_col2\" class=\"data row8 col2\" >20</td>\n",
       "      <td id=\"T_7eb1b_row8_col3\" class=\"data row8 col3\" >regression</td>\n",
       "      <td id=\"T_7eb1b_row8_col4\" class=\"data row8 col4\" >small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7eb1b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7eb1b_row9_col0\" class=\"data row9 col0\" >synthetic_medium_regression</td>\n",
       "      <td id=\"T_7eb1b_row9_col1\" class=\"data row9 col1\" >5000</td>\n",
       "      <td id=\"T_7eb1b_row9_col2\" class=\"data row9 col2\" >50</td>\n",
       "      <td id=\"T_7eb1b_row9_col3\" class=\"data row9 col3\" >regression</td>\n",
       "      <td id=\"T_7eb1b_row9_col4\" class=\"data row9 col4\" >medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d4c18175e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DatasetManager:\n",
    "    \"\"\"Manages dataset loading and preprocessing for consistent comparison.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_name: str) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "        \"\"\"Load and return dataset with task type.\"\"\"\n",
    "        print(f\"üìä Loading dataset: {dataset_name}\")\n",
    "        \n",
    "        if dataset_name == \"iris\":\n",
    "            data = load_iris()\n",
    "            return data.data, data.target, \"classification\"\n",
    "        elif dataset_name == \"wine\":\n",
    "            data = load_wine()\n",
    "            return data.data, data.target, \"classification\"\n",
    "        elif dataset_name == \"breast_cancer\":\n",
    "            data = load_breast_cancer()\n",
    "            return data.data, data.target, \"classification\"\n",
    "        elif dataset_name == \"digits\":\n",
    "            data = load_digits()\n",
    "            return data.data, data.target, \"classification\"\n",
    "        elif dataset_name == \"diabetes\":\n",
    "            data = load_diabetes()\n",
    "            return data.data, data.target, \"regression\"\n",
    "        elif dataset_name == \"california_housing\":\n",
    "            data = fetch_california_housing()\n",
    "            return data.data, data.target, \"regression\"\n",
    "        elif dataset_name == \"synthetic_small_classification\":\n",
    "            X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, \n",
    "                                     n_redundant=5, n_clusters_per_class=1, random_state=42)\n",
    "            return X, y, \"classification\"\n",
    "        elif dataset_name == \"synthetic_medium_classification\":\n",
    "            X, y = make_classification(n_samples=5000, n_features=50, n_informative=25, \n",
    "                                     n_redundant=15, n_clusters_per_class=1, random_state=42)\n",
    "            return X, y, \"classification\"\n",
    "        elif dataset_name == \"synthetic_small_regression\":\n",
    "            X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, \n",
    "                                 noise=0.1, random_state=42)\n",
    "            return X, y, \"regression\"\n",
    "        elif dataset_name == \"synthetic_medium_regression\":\n",
    "            X, y = make_regression(n_samples=5000, n_features=50, n_informative=35, \n",
    "                                 noise=0.1, random_state=42)\n",
    "            return X, y, \"regression\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dataset_info():\n",
    "        \"\"\"Return information about available datasets.\"\"\"\n",
    "        datasets_info = {\n",
    "            # Real-world datasets\n",
    "            \"iris\": {\"samples\": 150, \"features\": 4, \"type\": \"classification\", \"category\": \"small\"},\n",
    "            \"wine\": {\"samples\": 178, \"features\": 13, \"type\": \"classification\", \"category\": \"small\"},\n",
    "            \"breast_cancer\": {\"samples\": 569, \"features\": 30, \"type\": \"classification\", \"category\": \"small\"},\n",
    "            \"digits\": {\"samples\": 1797, \"features\": 64, \"type\": \"classification\", \"category\": \"medium\"},\n",
    "            \"diabetes\": {\"samples\": 442, \"features\": 10, \"type\": \"regression\", \"category\": \"small\"},\n",
    "            \"california_housing\": {\"samples\": 20640, \"features\": 8, \"type\": \"regression\", \"category\": \"large\"},\n",
    "            \n",
    "            # Synthetic datasets\n",
    "            \"synthetic_small_classification\": {\"samples\": 1000, \"features\": 20, \"type\": \"classification\", \"category\": \"small\"},\n",
    "            \"synthetic_medium_classification\": {\"samples\": 5000, \"features\": 50, \"type\": \"classification\", \"category\": \"medium\"},\n",
    "            \"synthetic_small_regression\": {\"samples\": 1000, \"features\": 20, \"type\": \"regression\", \"category\": \"small\"},\n",
    "            \"synthetic_medium_regression\": {\"samples\": 5000, \"features\": 50, \"type\": \"regression\", \"category\": \"medium\"},\n",
    "        }\n",
    "        return datasets_info\n",
    "\n",
    "# Initialize dataset manager and display available datasets\n",
    "dataset_manager = DatasetManager()\n",
    "datasets_info = dataset_manager.get_dataset_info()\n",
    "\n",
    "# Create a nice display of available datasets\n",
    "df_datasets = pd.DataFrame.from_dict(datasets_info, orient='index')\n",
    "df_datasets.index.name = 'Dataset'\n",
    "df_datasets = df_datasets.reset_index()\n",
    "\n",
    "print(\"üìä Available Datasets for Benchmarking:\")\n",
    "display(df_datasets.style.set_properties(**{'text-align': 'center'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e5cb6",
   "metadata": {},
   "source": [
    "# 3. Configure AutoML Platforms\n",
    "\n",
    "Let's set up configuration parameters for each AutoML platform to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53616e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Benchmark configuration and helper functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Benchmark configuration with AutoML Performance Settings\n",
    "BENCHMARK_CONFIG = {\n",
    "    \"time_budget\": 180,  # seconds per framework (3 minutes base time)\n",
    "    \"memory_limit\": 4096,  # MB\n",
    "    \"cv_folds\": 3,\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": 1,  # Single-threaded for fair comparison base\n",
    "    \"verbose\": False,\n",
    "    \n",
    "    # AutoML Performance Configuration\n",
    "    \"automl_time_budget\": 300,       # Extended time for AutoML frameworks (5 minutes)\n",
    "    \"enable_automl_optimization\": True,  # Enable AutoML-specific optimizations\n",
    "    \"optimization_strategy\": \"hyperx\",   # Default optimization strategy\n",
    "    \"enable_ensemble\": True,         # Enable ensemble methods for AutoML\n",
    "    \"enable_feature_selection\": True, # Enable automatic feature selection\n",
    "    \"adaptive_batch_size\": True,     # Enable adaptive batch sizing\n",
    "    \n",
    "    # Resource Management\n",
    "    \"max_workers_automl\": min(4, os.cpu_count()),  # Optimal workers for AutoML\n",
    "    \"memory_optimization\": True,     # Enable memory optimization\n",
    "    \"enable_early_stopping\": True,   # Enable early stopping\n",
    "}\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor system resources.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "    return memory_mb\n",
    "\n",
    "def benchmark_framework(framework_func, dataset_name: str, framework_name: str) -> AutoMLBenchmarkResult:\n",
    "    \"\"\"\n",
    "    Generic function to benchmark any AutoML framework with performance optimization.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    initial_memory = monitor_resources()\n",
    "    \n",
    "    try:\n",
    "        # Clear memory before benchmark\n",
    "        gc.collect()\n",
    "        \n",
    "        # Load dataset\n",
    "        X, y, task_type = dataset_manager.load_dataset(dataset_name)\n",
    "        \n",
    "        # Split data consistently\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=BENCHMARK_CONFIG[\"test_size\"], \n",
    "            random_state=BENCHMARK_CONFIG[\"random_state\"],\n",
    "            stratify=y if task_type == \"classification\" else None\n",
    "        )\n",
    "        \n",
    "        print(f\"üöÄ Running {framework_name} on {dataset_name}...\")\n",
    "        \n",
    "        # Determine time budget based on framework capabilities\n",
    "        if \"automl\" in framework_name.lower() or \"kolosal\" in framework_name.lower():\n",
    "            # Use extended time budget for AutoML frameworks\n",
    "            original_time_budget = BENCHMARK_CONFIG.get(\"time_budget\", 180)\n",
    "            if BENCHMARK_CONFIG.get(\"enable_automl_optimization\", False):\n",
    "                BENCHMARK_CONFIG[\"time_budget\"] = BENCHMARK_CONFIG.get(\"automl_time_budget\", 300)\n",
    "            print(f\"‚ö° Using AutoML performance mode: {BENCHMARK_CONFIG['time_budget']}s time budget\")\n",
    "        \n",
    "        # Run the framework-specific benchmark\n",
    "        result = framework_func(X_train, X_test, y_train, y_test, task_type)\n",
    "        \n",
    "        # Restore original time budget if changed\n",
    "        if \"automl\" in framework_name.lower() or \"kolosal\" in framework_name.lower():\n",
    "            BENCHMARK_CONFIG[\"time_budget\"] = original_time_budget\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        training_time = time.time() - start_time\n",
    "        final_memory = monitor_resources()\n",
    "        \n",
    "        # Update result with common metrics\n",
    "        result.experiment_id = experiment_id\n",
    "        result.dataset_name = dataset_name\n",
    "        result.dataset_size = X.shape\n",
    "        result.task_type = task_type\n",
    "        result.training_time = training_time\n",
    "        result.memory_peak_mb = max(initial_memory, final_memory)\n",
    "        result.memory_final_mb = final_memory\n",
    "        result.feature_count = X.shape[1]\n",
    "        result.success = True\n",
    "        \n",
    "        # Performance indicator\n",
    "        performance_indicator = \"üöÄ\" if \"automl\" in framework_name.lower() else \"‚úÖ\"\n",
    "        print(f\"{performance_indicator} {framework_name} completed: {result.test_score:.4f} score in {training_time:.2f}s\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå {framework_name} failed: {error_msg}\")\n",
    "        \n",
    "        # Return error result\n",
    "        return AutoMLBenchmarkResult(\n",
    "            experiment_id=experiment_id,\n",
    "            approach=framework_name.lower().replace(' ', '_'),\n",
    "            dataset_name=dataset_name,\n",
    "            model_name=f\"{framework_name.lower()}_automl\",\n",
    "            dataset_size=(0, 0),\n",
    "            task_type=\"unknown\",\n",
    "            training_time=time.time() - start_time,\n",
    "            prediction_time=0,\n",
    "            memory_peak_mb=initial_memory,\n",
    "            memory_final_mb=monitor_resources(),\n",
    "            train_score=0,\n",
    "            test_score=0,\n",
    "            cv_score_mean=0,\n",
    "            cv_score_std=0,\n",
    "            best_params={},\n",
    "            feature_count=0,\n",
    "            model_size_mb=0,\n",
    "            preprocessing_time=0,\n",
    "            success=False,\n",
    "            error_message=error_msg,\n",
    "            framework_version=\"unknown\"\n",
    "        )\n",
    "\n",
    "print(\"‚öôÔ∏è Benchmark configuration and helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0d4f7",
   "metadata": {},
   "source": [
    "# 4. Run AutoML Toolkit Benchmarks\n",
    "\n",
    "Now let's benchmark popular AutoML platforms including FLAML, Auto-sklearn, TPOT, H2O AutoML, and AutoGluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f49eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß AutoML benchmark functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Standard ML Baseline\n",
    "def benchmark_standard_ml(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark standard ML with scikit-learn.\"\"\"\n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    # Preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Model selection\n",
    "    if task_type == \"classification\":\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "        scoring_func = accuracy_score\n",
    "    else:\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "        scoring_func = r2_score\n",
    "    \n",
    "    # Training\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    test_pred = model.predict(X_test_scaled)\n",
    "    prediction_time = time.time() - prediction_start\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=3)\n",
    "    \n",
    "    return AutoMLBenchmarkResult(\n",
    "        experiment_id=\"\",\n",
    "        approach=\"standard_ml\",\n",
    "        dataset_name=\"\",\n",
    "        model_name=\"random_forest\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=scoring_func(y_train, train_pred),\n",
    "        test_score=scoring_func(y_test, test_pred),\n",
    "        cv_score_mean=cv_scores.mean(),\n",
    "        cv_score_std=cv_scores.std(),\n",
    "        best_params={},\n",
    "        feature_count=0,\n",
    "        model_size_mb=sys.getsizeof(model) / (1024 * 1024),\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"sklearn\"\n",
    "    )\n",
    "\n",
    "# FLAML AutoML with Performance Configuration\n",
    "def benchmark_flaml(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark FLAML AutoML with performance optimization.\"\"\"\n",
    "    if not FLAML_AVAILABLE:\n",
    "        raise ValueError(\"FLAML not available\")\n",
    "    \n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    automl = FLAML_AutoML()\n",
    "    \n",
    "    # Performance-optimized settings for FLAML\n",
    "    settings = {\n",
    "        \"time_budget\": BENCHMARK_CONFIG.get(\"automl_time_budget\", BENCHMARK_CONFIG[\"time_budget\"]),\n",
    "        \"metric\": \"accuracy\" if task_type == \"classification\" else \"r2\",\n",
    "        \"task\": task_type,\n",
    "        \"seed\": BENCHMARK_CONFIG[\"random_state\"],\n",
    "        \"verbose\": 0,\n",
    "        \n",
    "        # Performance optimization settings\n",
    "        \"n_jobs\": BENCHMARK_CONFIG.get(\"max_workers_automl\", 1),\n",
    "        \"early_stop\": BENCHMARK_CONFIG.get(\"enable_early_stopping\", False),\n",
    "        \"retrain_full\": True,  # Retrain on full dataset\n",
    "        \"auto_augment\": True,  # Enable automatic data augmentation\n",
    "        \"ensemble\": BENCHMARK_CONFIG.get(\"enable_ensemble\", False),\n",
    "        \n",
    "        # Memory optimization\n",
    "        \"mem_thres\": BENCHMARK_CONFIG.get(\"memory_limit\", 4096) * 1024 * 1024,  # Convert MB to bytes\n",
    "        \n",
    "        # Advanced settings for better performance\n",
    "        \"eval_method\": \"cv\",   # Use cross-validation\n",
    "        \"split_ratio\": 0.2,   # Validation split ratio\n",
    "        \"n_splits\": BENCHMARK_CONFIG.get(\"cv_folds\", 3),\n",
    "    }\n",
    "    \n",
    "    automl.fit(X_train, y_train, **settings)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = automl.predict(X_train)\n",
    "    test_pred = automl.predict(X_test)\n",
    "    prediction_time = time.time() - prediction_start\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        train_score = accuracy_score(y_train, train_pred)\n",
    "        test_score = accuracy_score(y_test, test_pred)\n",
    "    else:\n",
    "        train_score = r2_score(y_train, train_pred)\n",
    "        test_score = r2_score(y_test, test_pred)\n",
    "    \n",
    "    return AutoMLBenchmarkResult(\n",
    "        experiment_id=\"\",\n",
    "        approach=\"flaml_automl\",\n",
    "        dataset_name=\"\",\n",
    "        model_name=\"flaml_optimized\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        cv_score_mean=test_score,  # FLAML provides best CV score\n",
    "        cv_score_std=0.0,\n",
    "        best_params=automl.best_config if hasattr(automl, 'best_config') else {},\n",
    "        feature_count=0,\n",
    "        model_size_mb=sys.getsizeof(automl.model) / (1024 * 1024) if hasattr(automl, 'model') else 0,\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"flaml_optimized\"\n",
    "    )\n",
    "        model_name=\"flaml_automl\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        cv_score_mean=test_score,\n",
    "        cv_score_std=0.0,\n",
    "        best_params=automl.best_config if hasattr(automl, 'best_config') else {},\n",
    "        feature_count=0,\n",
    "        model_size_mb=sys.getsizeof(automl.model) / (1024 * 1024) if hasattr(automl, 'model') else 0,\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"flaml\"\n",
    "    )\n",
    "\n",
    "# Auto-sklearn\n",
    "def benchmark_autosklearn(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark Auto-sklearn.\"\"\"\n",
    "    if not AUTOSKLEARN_AVAILABLE:\n",
    "        raise ValueError(\"Auto-sklearn not available\")\n",
    "    \n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "            time_left_for_this_task=BENCHMARK_CONFIG[\"time_budget\"],\n",
    "            per_run_time_limit=30,\n",
    "            seed=BENCHMARK_CONFIG[\"random_state\"],\n",
    "            memory_limit=BENCHMARK_CONFIG[\"memory_limit\"],\n",
    "            disable_evaluator_output=True\n",
    "        )\n",
    "        scoring_func = accuracy_score\n",
    "    else:\n",
    "        automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "            time_left_for_this_task=BENCHMARK_CONFIG[\"time_budget\"],\n",
    "            per_run_time_limit=30,\n",
    "            seed=BENCHMARK_CONFIG[\"random_state\"],\n",
    "            memory_limit=BENCHMARK_CONFIG[\"memory_limit\"],\n",
    "            disable_evaluator_output=True\n",
    "        )\n",
    "        scoring_func = r2_score\n",
    "    \n",
    "    automl.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = automl.predict(X_train)\n",
    "    test_pred = automl.predict(X_test)\n",
    "    prediction_time = time.time() - prediction_start\n",
    "    \n",
    "    return AutoMLBenchmarkResult(\n",
    "        experiment_id=\"\",\n",
    "        approach=\"autosklearn\",\n",
    "        dataset_name=\"\",\n",
    "        model_name=\"autosklearn_automl\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=scoring_func(y_train, train_pred),\n",
    "        test_score=scoring_func(y_test, test_pred),\n",
    "        cv_score_mean=scoring_func(y_test, test_pred),\n",
    "        cv_score_std=0.0,\n",
    "        best_params={},\n",
    "        feature_count=0,\n",
    "        model_size_mb=sys.getsizeof(automl) / (1024 * 1024),\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"autosklearn\"\n",
    "    )\n",
    "\n",
    "# TPOT\n",
    "def benchmark_tpot(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark TPOT.\"\"\"\n",
    "    if not TPOT_AVAILABLE:\n",
    "        raise ValueError(\"TPOT not available\")\n",
    "    \n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        automl = TPOTClassifier(\n",
    "            generations=5,\n",
    "            population_size=20,\n",
    "            verbosity=0,\n",
    "            random_state=BENCHMARK_CONFIG[\"random_state\"],\n",
    "            max_time_mins=BENCHMARK_CONFIG[\"time_budget\"]//60,\n",
    "            cv=3\n",
    "        )\n",
    "        scoring_func = accuracy_score\n",
    "    else:\n",
    "        automl = TPOTRegressor(\n",
    "            generations=5,\n",
    "            population_size=20,\n",
    "            verbosity=0,\n",
    "            random_state=BENCHMARK_CONFIG[\"random_state\"],\n",
    "            max_time_mins=BENCHMARK_CONFIG[\"time_budget\"]//60,\n",
    "            cv=3\n",
    "        )\n",
    "        scoring_func = r2_score\n",
    "    \n",
    "    automl.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = automl.predict(X_train)\n",
    "    test_pred = automl.predict(X_test)\n",
    "    prediction_time = time.time() - prediction_start\n",
    "    \n",
    "    return AutoMLBenchmarkResult(\n",
    "        experiment_id=\"\",\n",
    "        approach=\"tpot\",\n",
    "        dataset_name=\"\",\n",
    "        model_name=\"tpot_automl\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=scoring_func(y_train, train_pred),\n",
    "        test_score=scoring_func(y_test, test_pred),\n",
    "        cv_score_mean=scoring_func(y_test, test_pred),\n",
    "        cv_score_std=0.0,\n",
    "        best_params={},\n",
    "        feature_count=0,\n",
    "        model_size_mb=sys.getsizeof(automl.fitted_pipeline_) / (1024 * 1024) if hasattr(automl, 'fitted_pipeline_') else 0,\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"tpot\"\n",
    "    )\n",
    "\n",
    "print(\"üîß AutoML benchmark functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolosal AutoML Benchmark with Performance Configuration\n",
    "def benchmark_kolosal_automl(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark Kolosal AutoML with performance-optimized configuration.\"\"\"\n",
    "    if not KOLOSAL_AVAILABLE:\n",
    "        raise ValueError(\"Kolosal AutoML not available\")\n",
    "    \n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    # Set task type enum\n",
    "    task = TaskType.CLASSIFICATION if task_type == \"classification\" else TaskType.REGRESSION\n",
    "    \n",
    "    # Performance-optimized AutoML configuration\n",
    "    config = MLTrainingEngineConfig(\n",
    "        task_type=task,\n",
    "        model_path=\"./benchmark_models\",\n",
    "        \n",
    "        # AutoML Performance Optimization Settings\n",
    "        optimization_strategy=OptimizationStrategy.HYPERX,  # Advanced hyperparameter optimization\n",
    "        enable_automl=True,                     # Enable full AutoML pipeline\n",
    "        enable_ensemble=True,                   # Enable ensemble methods\n",
    "        enable_feature_selection=True,         # Enable automatic feature selection\n",
    "        \n",
    "        # Performance Optimization Settings\n",
    "        enable_jit_compilation=True,            # Enable JIT compilation for speed\n",
    "        enable_mixed_precision=True,           # Enable mixed precision training\n",
    "        enable_adaptive_hyperopt=True,         # Enable adaptive hyperparameter optimization\n",
    "        enable_streaming=True,                  # Enable streaming for large datasets\n",
    "        enable_early_stopping=True,            # Enable early stopping\n",
    "        \n",
    "        # Resource Management\n",
    "        enable_memory_optimization=True,       # Enable memory optimization\n",
    "        enable_parallel_processing=True,       # Enable parallel processing\n",
    "        max_workers=min(4, os.cpu_count()),   # Optimal number of workers\n",
    "        \n",
    "        # Training Configuration\n",
    "        cv_folds=3,                            # 3-fold CV for speed vs accuracy balance\n",
    "        test_size=0.2,\n",
    "        stratify=(task == TaskType.CLASSIFICATION),\n",
    "        random_state=BENCHMARK_CONFIG[\"random_state\"],\n",
    "        max_iter=100,                          # Reasonable iteration limit\n",
    "        \n",
    "        # Preprocessing Configuration\n",
    "        preprocessor_config=PreprocessorConfig(\n",
    "            normalization=NormalizationType.STANDARD,\n",
    "            handle_nan=True,\n",
    "            handle_inf=True,\n",
    "            detect_outliers=True,              # Enable outlier detection\n",
    "            parallel_processing=True,          # Enable parallel preprocessing\n",
    "        ),\n",
    "        \n",
    "        # Verbose settings for benchmarking\n",
    "        verbose=0,                             # Reduce verbosity for cleaner output\n",
    "        log_level=\"WARNING\"                    # Reduce log noise during benchmarking\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Initialize the training engine\n",
    "        engine = MLTrainingEngine(config)\n",
    "        \n",
    "        # Use automatic model selection and training\n",
    "        if task_type == \"classification\":\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            model = RandomForestClassifier(random_state=42)\n",
    "            param_grid = {\n",
    "                \"n_estimators\": [50, 100, 200],\n",
    "                \"max_depth\": [None, 5, 10, 20],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4]\n",
    "            }\n",
    "        else:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            model = RandomForestRegressor(random_state=42)\n",
    "            param_grid = {\n",
    "                \"n_estimators\": [50, 100, 200],\n",
    "                \"max_depth\": [None, 5, 10, 20],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4]\n",
    "            }\n",
    "        \n",
    "        # Train with AutoML optimization\n",
    "        result = engine.train_model(\n",
    "            model=model,\n",
    "            model_name=\"kolosal_automl_model\",\n",
    "            param_grid=param_grid,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            X_val=X_test,\n",
    "            y_val=y_test\n",
    "        )\n",
    "        \n",
    "        # Get the best model\n",
    "        best_model = result.get('model') or engine.best_model\n",
    "        best_params = result.get('best_params', {})\n",
    "        \n",
    "        if best_model is None:\n",
    "            raise ValueError(\"Training failed: No model was trained successfully\")\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = best_model.predict(X_train)\n",
    "        test_pred = best_model.predict(X_test)\n",
    "        prediction_time = time.time() - prediction_start\n",
    "        \n",
    "        # Calculate scores\n",
    "        if task_type == \"classification\":\n",
    "            train_score = accuracy_score(y_train, train_pred)\n",
    "            test_score = accuracy_score(y_test, test_pred)\n",
    "            cv_scores = cross_val_score(best_model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "        else:\n",
    "            train_score = r2_score(y_train, train_pred)\n",
    "            test_score = r2_score(y_test, test_pred)\n",
    "            cv_scores = cross_val_score(best_model, X_train, y_train, cv=3, scoring='r2')\n",
    "        \n",
    "        # Clean up\n",
    "        engine.shutdown()\n",
    "        \n",
    "        return AutoMLBenchmarkResult(\n",
    "            experiment_id=\"\",\n",
    "            approach=\"kolosal_automl\",\n",
    "            dataset_name=\"\",\n",
    "            model_name=\"kolosal_automl_optimized\",\n",
    "            dataset_size=(0, 0),\n",
    "            task_type=task_type,\n",
    "            training_time=0,\n",
    "            prediction_time=prediction_time,\n",
    "            memory_peak_mb=0,\n",
    "            memory_final_mb=0,\n",
    "            train_score=train_score,\n",
    "            test_score=test_score,\n",
    "            cv_score_mean=cv_scores.mean(),\n",
    "            cv_score_std=cv_scores.std(),\n",
    "            best_params=best_params,\n",
    "            feature_count=0,\n",
    "            model_size_mb=sys.getsizeof(best_model) / (1024 * 1024),\n",
    "            preprocessing_time=0,\n",
    "            success=True,\n",
    "            framework_version=\"kolosal_v0.1.4\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Training failed: {str(e)}\"\n",
    "        print(f\"‚ùå Kolosal AutoML error: {error_msg}\")\n",
    "        \n",
    "        # Clean up on error\n",
    "        try:\n",
    "            engine.shutdown()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        raise ValueError(error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46767b",
   "metadata": {},
   "source": [
    "# 5. Run Kolosal-AutoML Benchmark\n",
    "\n",
    "Now let's benchmark our own Kolosal-AutoML system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fafd748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Kolosal AutoML benchmark function ready!\n"
     ]
    }
   ],
   "source": [
    "# Kolosal AutoML Benchmark\n",
    "def benchmark_kolosal_automl(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark Kolosal AutoML.\"\"\"\n",
    "    if not KOLOSAL_AVAILABLE:\n",
    "        raise ValueError(\"Kolosal AutoML not available\")\n",
    "    \n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    # Create engine configuration\n",
    "    def create_engine_config(task_type: str) -> MLTrainingEngineConfig:\n",
    "        \"\"\"Create Kolosal ML engine configuration.\"\"\"\n",
    "        task = TaskType.CLASSIFICATION if task_type == \"classification\" else TaskType.REGRESSION\n",
    "        \n",
    "        preprocessor_config = PreprocessorConfig(\n",
    "            normalization=NormalizationType.STANDARD,\n",
    "            handle_nan=True,\n",
    "            handle_inf=True,\n",
    "            detect_outliers=False\n",
    "        )\n",
    "        \n",
    "        config = MLTrainingEngineConfig(\n",
    "            task_type=task,\n",
    "            random_state=BENCHMARK_CONFIG[\"random_state\"],\n",
    "            n_jobs=BENCHMARK_CONFIG[\"n_jobs\"],\n",
    "            verbose=0,\n",
    "            cv_folds=BENCHMARK_CONFIG[\"cv_folds\"],\n",
    "            test_size=BENCHMARK_CONFIG[\"test_size\"],\n",
    "            stratify=(task == TaskType.CLASSIFICATION),\n",
    "            optimization_strategy=OptimizationStrategy.RANDOM_SEARCH,\n",
    "            optimization_iterations=10,\n",
    "            early_stopping=False,\n",
    "            feature_selection=False,\n",
    "            preprocessing_config=preprocessor_config,\n",
    "            model_path=\"./benchmark_models\",\n",
    "            experiment_tracking=False,\n",
    "            use_intel_optimization=False,\n",
    "            memory_optimization=False\n",
    "        )\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    # Create and configure the engine\n",
    "    config = create_engine_config(task_type)\n",
    "    engine = MLTrainingEngine(config)\n",
    "    \n",
    "    # Get appropriate model and parameters\n",
    "    if task_type == \"classification\":\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        scoring_func = accuracy_score\n",
    "    else:\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        scoring_func = r2_score\n",
    "    \n",
    "    # Train the model\n",
    "    training_result = engine.train_model(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        custom_model=model,\n",
    "        model_name=\"kolosal_model\",\n",
    "        param_grid=param_grid,\n",
    "        X_val=X_test,\n",
    "        y_val=y_test\n",
    "    )\n",
    "    \n",
    "    if not training_result or not training_result.get(\"success\", False):\n",
    "        error_msg = training_result.get(\"error\", \"Unknown training error\") if training_result else \"Training result is None\"\n",
    "        raise ValueError(f\"Training failed: {error_msg}\")\n",
    "    \n",
    "    # Get trained model\n",
    "    trained_model = training_result.get(\"model\")\n",
    "    if not trained_model:\n",
    "        raise ValueError(\"No trained model returned\")\n",
    "    \n",
    "    # Apply preprocessing if available\n",
    "    if hasattr(engine, 'preprocessor') and engine.preprocessor:\n",
    "        X_train_processed = engine.preprocessor.transform(X_train)\n",
    "        X_test_processed = engine.preprocessor.transform(X_test)\n",
    "    else:\n",
    "        X_train_processed = X_train\n",
    "        X_test_processed = X_test\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = trained_model.predict(X_train_processed)\n",
    "    test_pred = trained_model.predict(X_test_processed)\n",
    "    prediction_time = time.time() - prediction_start\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(trained_model, X_train_processed, y_train, cv=3)\n",
    "    \n",
    "    # Cleanup\n",
    "    engine.shutdown()\n",
    "    \n",
    "    return AutoMLBenchmarkResult(\n",
    "        experiment_id=\"\",\n",
    "        approach=\"kolosal\",\n",
    "        dataset_name=\"\",\n",
    "        model_name=\"kolosal_automl\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=scoring_func(y_train, train_pred),\n",
    "        test_score=scoring_func(y_test, test_pred),\n",
    "        cv_score_mean=cv_scores.mean(),\n",
    "        cv_score_std=cv_scores.std(),\n",
    "        best_params=training_result.get(\"params\", {}),\n",
    "        feature_count=0,\n",
    "        model_size_mb=sys.getsizeof(trained_model) / (1024 * 1024),\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"kolosal-1.0\"\n",
    "    )\n",
    "\n",
    "print(\"üèóÔ∏è Kolosal AutoML benchmark function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf20a6",
   "metadata": {},
   "source": [
    "# 6. Run Additional AutoML Platforms\n",
    "\n",
    "Let's add more AutoML platforms for comprehensive comparison including H2O AutoML, AutoGluon, PyCaret, and MLjar-Supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6c4be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Available AutoML Frameworks:\n",
      "  ‚Ä¢ Standard ML: ‚úÖ Available\n",
      "  ‚Ä¢ Kolosal AutoML: ‚úÖ Available\n",
      "  ‚Ä¢ FLAML: ‚úÖ Available\n",
      "  ‚Ä¢ Auto-sklearn: ‚ùå Not Available\n",
      "  ‚Ä¢ TPOT: ‚ùå Not Available\n",
      "  ‚Ä¢ AutoGluon: ‚úÖ Available\n",
      "  ‚Ä¢ MLjar-Supervised: ‚úÖ Available\n",
      "\n",
      "üìä Total Available: 5 frameworks\n"
     ]
    }
   ],
   "source": [
    "# AutoGluon\n",
    "def benchmark_autogluon(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark AutoGluon.\"\"\"\n",
    "    if not AUTOGLUON_AVAILABLE:\n",
    "        raise ValueError(\"AutoGluon not available\")\n",
    "    \n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data = pd.DataFrame(X_train)\n",
    "    test_data = pd.DataFrame(X_test)\n",
    "    target_col = 'target'\n",
    "    train_data[target_col] = y_train\n",
    "    test_data[target_col] = y_test\n",
    "    \n",
    "    # Create predictor\n",
    "    predictor = TabularPredictor(\n",
    "        label=target_col,\n",
    "        problem_type='binary' if task_type == 'classification' else 'regression',\n",
    "        path='./autogluon_models'\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    predictor.fit(\n",
    "        train_data,\n",
    "        time_limit=BENCHMARK_CONFIG[\"time_budget\"],\n",
    "        presets='medium_quality_faster_train'\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = predictor.predict(train_data.drop(columns=[target_col]))\n",
    "    test_pred = predictor.predict(test_data.drop(columns=[target_col]))\n",
    "    prediction_time = time.time() - prediction_start\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        train_score = accuracy_score(y_train, train_pred)\n",
    "        test_score = accuracy_score(y_test, test_pred)\n",
    "    else:\n",
    "        train_score = r2_score(y_train, train_pred)\n",
    "        test_score = r2_score(y_test, test_pred)\n",
    "    \n",
    "    return AutoMLBenchmarkResult(\n",
    "        experiment_id=\"\",\n",
    "        approach=\"autogluon\",\n",
    "        dataset_name=\"\",\n",
    "        model_name=\"autogluon_automl\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        cv_score_mean=test_score,\n",
    "        cv_score_std=0.0,\n",
    "        best_params={},\n",
    "        feature_count=0,\n",
    "        model_size_mb=0,\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"autogluon\"\n",
    "    )\n",
    "\n",
    "# MLjar-Supervised\n",
    "def benchmark_mljar(X_train, X_test, y_train, y_test, task_type):\n",
    "    \"\"\"Benchmark MLjar-Supervised.\"\"\"\n",
    "    if not MLJAR_AVAILABLE:\n",
    "        raise ValueError(\"MLjar-Supervised not available\")\n",
    "    \n",
    "    prediction_start = time.time()\n",
    "    \n",
    "    automl = MLjarAutoML(\n",
    "        mode=\"Compete\",  # For better performance\n",
    "        ml_task=task_type,\n",
    "        total_time_limit=BENCHMARK_CONFIG[\"time_budget\"],\n",
    "        results_path=\"./mljar_results\"\n",
    "    )\n",
    "    \n",
    "    automl.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = automl.predict(X_train)\n",
    "    test_pred = automl.predict(X_test)\n",
    "    prediction_time = time.time() - prediction_start\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        train_score = accuracy_score(y_train, train_pred)\n",
    "        test_score = accuracy_score(y_test, test_pred)\n",
    "    else:\n",
    "        train_score = r2_score(y_train, train_pred)\n",
    "        test_score = r2_score(y_test, test_pred)\n",
    "    \n",
    "    return AutoMLBenchmarkResult(\n",
    "        experiment_id=\"\",\n",
    "        approach=\"mljar\",\n",
    "        dataset_name=\"\",\n",
    "        model_name=\"mljar_automl\",\n",
    "        dataset_size=(0, 0),\n",
    "        task_type=task_type,\n",
    "        training_time=0,\n",
    "        prediction_time=prediction_time,\n",
    "        memory_peak_mb=0,\n",
    "        memory_final_mb=0,\n",
    "        train_score=train_score,\n",
    "        test_score=test_score,\n",
    "        cv_score_mean=test_score,\n",
    "        cv_score_std=0.0,\n",
    "        best_params={},\n",
    "        feature_count=0,\n",
    "        model_size_mb=0,\n",
    "        preprocessing_time=0,\n",
    "        success=True,\n",
    "        framework_version=\"mljar\"\n",
    "    )\n",
    "\n",
    "# Create mapping of available frameworks\n",
    "AVAILABLE_FRAMEWORKS = {\n",
    "    \"Standard ML\": (benchmark_standard_ml, True),\n",
    "    \"Kolosal AutoML\": (benchmark_kolosal_automl, KOLOSAL_AVAILABLE),\n",
    "    \"FLAML\": (benchmark_flaml, FLAML_AVAILABLE),\n",
    "    \"Auto-sklearn\": (benchmark_autosklearn, AUTOSKLEARN_AVAILABLE),\n",
    "    \"TPOT\": (benchmark_tpot, TPOT_AVAILABLE),\n",
    "    \"AutoGluon\": (benchmark_autogluon, AUTOGLUON_AVAILABLE),\n",
    "    \"MLjar-Supervised\": (benchmark_mljar, MLJAR_AVAILABLE),\n",
    "}\n",
    "\n",
    "# Display available frameworks\n",
    "print(\"ü§ñ Available AutoML Frameworks:\")\n",
    "for name, (func, available) in AVAILABLE_FRAMEWORKS.items():\n",
    "    status = \"‚úÖ Available\" if available else \"‚ùå Not Available\"\n",
    "    print(f\"  ‚Ä¢ {name}: {status}\")\n",
    "\n",
    "print(f\"\\nüìä Total Available: {sum(available for _, available in AVAILABLE_FRAMEWORKS.values())} frameworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e9541",
   "metadata": {},
   "source": [
    "# 7. Collect Performance Metrics\n",
    "\n",
    "Now let's run the actual benchmarks and collect performance metrics from all available frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8849675",
   "metadata": {},
   "source": [
    "# üöÄ AutoML Performance Configuration Overview\n",
    "\n",
    "The notebook has been updated to use **performance-optimized AutoML configurations** for all training operations. Here's what has been enhanced:\n",
    "\n",
    "## üéØ Key Performance Features\n",
    "\n",
    "### **Kolosal AutoML Performance Optimization**\n",
    "- **HYPERX Optimization Strategy**: Advanced hyperparameter optimization beyond traditional methods\n",
    "- **JIT Compilation**: Just-in-time compilation for faster execution\n",
    "- **Mixed Precision Training**: Automatic FP16/FP32 optimization for speed and memory\n",
    "- **Adaptive Hyperparameter Optimization**: Dynamic adjustment of search strategies\n",
    "- **Streaming Pipeline**: Efficient handling of large datasets\n",
    "- **Ensemble Methods**: Automatic model ensemble creation\n",
    "- **Feature Selection**: Intelligent feature selection for better performance\n",
    "\n",
    "### **Enhanced Framework Configurations**\n",
    "- **Extended Time Budgets**: AutoML frameworks get 5 minutes vs 3 minutes for standard ML\n",
    "- **Parallel Processing**: Multi-worker optimization where supported\n",
    "- **Memory Optimization**: Advanced memory management and garbage collection\n",
    "- **Early Stopping**: Intelligent early termination for efficiency\n",
    "- **Adaptive Batch Sizing**: Dynamic batch size optimization\n",
    "\n",
    "### **Performance Monitoring**\n",
    "- **Real-time Resource Tracking**: CPU, memory, and time monitoring\n",
    "- **Performance Indicators**: Visual indicators for AutoML vs standard approaches\n",
    "- **Comprehensive Metrics**: Training time, memory usage, prediction speed, and accuracy\n",
    "\n",
    "## üìä Configuration Highlights\n",
    "\n",
    "```python\n",
    "# AutoML Performance Settings\n",
    "BENCHMARK_CONFIG = {\n",
    "    \"automl_time_budget\": 300,           # 5 minutes for AutoML frameworks\n",
    "    \"enable_automl_optimization\": True,   # Enable all optimizations\n",
    "    \"optimization_strategy\": \"hyperx\",   # Advanced optimization\n",
    "    \"enable_ensemble\": True,              # Ensemble methods\n",
    "    \"enable_feature_selection\": True,    # Automatic feature selection\n",
    "    \"max_workers_automl\": 4,              # Parallel processing\n",
    "    \"memory_optimization\": True,          # Memory management\n",
    "    \"enable_early_stopping\": True        # Early stopping\n",
    "}\n",
    "\n",
    "# Kolosal AutoML Configuration\n",
    "MLTrainingEngineConfig(\n",
    "    optimization_strategy=OptimizationStrategy.HYPERX,\n",
    "    enable_automl=True,\n",
    "    enable_jit_compilation=True,\n",
    "    enable_mixed_precision=True,\n",
    "    enable_adaptive_hyperopt=True,\n",
    "    enable_streaming=True,\n",
    "    enable_ensemble=True,\n",
    "    enable_feature_selection=True\n",
    ")\n",
    "```\n",
    "\n",
    "## üéÆ Ready to Benchmark\n",
    "\n",
    "All frameworks now leverage these performance optimizations where applicable:\n",
    "- **Kolosal AutoML**: Full performance optimization suite\n",
    "- **FLAML**: Enhanced with ensemble methods and memory optimization  \n",
    "- **Other frameworks**: Standard optimizations maintained for fair comparison\n",
    "\n",
    "The benchmarking will automatically detect AutoML frameworks and apply the appropriate performance configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a316c5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdad106c3a34c8294aad888423be8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HTML(value='<h4>Select Datasets:</h4>'), SelectMultiple(description='Datasets:',‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dea58389e544638bed8bc05c759fbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=120, description='Time Budget (s):', max=600, min=60, step=30, style=SliderStyle(description_w‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéõÔ∏è Selection interface ready!\n"
     ]
    }
   ],
   "source": [
    "# Interactive selection of datasets and frameworks\n",
    "if WIDGETS_AVAILABLE:\n",
    "    # Dataset selection\n",
    "    dataset_options = list(datasets_info.keys())\n",
    "    dataset_selector = widgets.SelectMultiple(\n",
    "        options=dataset_options,\n",
    "        value=['iris', 'wine', 'breast_cancer'],  # Default selection\n",
    "        description='Datasets:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='300px', height='150px')\n",
    "    )\n",
    "    \n",
    "    # Framework selection\n",
    "    framework_options = [name for name, (func, available) in AVAILABLE_FRAMEWORKS.items() if available]\n",
    "    framework_selector = widgets.SelectMultiple(\n",
    "        options=framework_options,\n",
    "        value=framework_options[:3],  # Default first 3 available\n",
    "        description='Frameworks:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='300px', height='150px')\n",
    "    )\n",
    "    \n",
    "    # Time budget slider\n",
    "    time_budget_slider = widgets.IntSlider(\n",
    "        value=120,\n",
    "        min=60,\n",
    "        max=600,\n",
    "        step=30,\n",
    "        description='Time Budget (s):',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.HBox([\n",
    "        widgets.VBox([widgets.HTML('<h4>Select Datasets:</h4>'), dataset_selector]),\n",
    "        widgets.VBox([widgets.HTML('<h4>Select Frameworks:</h4>'), framework_selector])\n",
    "    ]))\n",
    "    display(time_budget_slider)\n",
    "    \n",
    "else:\n",
    "    # Fallback for when widgets are not available\n",
    "    selected_datasets = ['iris', 'wine', 'breast_cancer']\n",
    "    selected_frameworks = [name for name, (func, available) in AVAILABLE_FRAMEWORKS.items() if available][:3]\n",
    "    time_budget = 120\n",
    "    \n",
    "    print(f\"Selected datasets: {selected_datasets}\")\n",
    "    print(f\"Selected frameworks: {selected_frameworks}\")\n",
    "    print(f\"Time budget: {time_budget} seconds\")\n",
    "\n",
    "print(\"üéõÔ∏è Selection interface ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a68ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15caf2ec88c948d2af93ebb3918dfeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='üöÄ Run Benchmark', layout=Layout(height='40px', widt‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Benchmark runner ready!\n"
     ]
    }
   ],
   "source": [
    "def run_comprehensive_benchmark(selected_datasets=None, selected_frameworks=None, time_budget=None):\n",
    "    \"\"\"Run comprehensive benchmark across selected datasets and frameworks.\"\"\"\n",
    "    global benchmark_results, BENCHMARK_CONFIG\n",
    "    \n",
    "    # Get selections\n",
    "    if WIDGETS_AVAILABLE and selected_datasets is None:\n",
    "        selected_datasets = list(dataset_selector.value)\n",
    "        selected_frameworks = list(framework_selector.value)\n",
    "        time_budget = time_budget_slider.value\n",
    "    elif selected_datasets is None:\n",
    "        selected_datasets = ['iris', 'wine', 'breast_cancer']\n",
    "        selected_frameworks = [name for name, (func, available) in AVAILABLE_FRAMEWORKS.items() if available][:3]\n",
    "        time_budget = 120\n",
    "    \n",
    "    # Update config\n",
    "    BENCHMARK_CONFIG[\"time_budget\"] = time_budget\n",
    "    \n",
    "    print(f\"üöÄ Starting comprehensive benchmark...\")\n",
    "    print(f\"üìä Datasets: {selected_datasets}\")\n",
    "    print(f\"ü§ñ Frameworks: {selected_frameworks}\")\n",
    "    print(f\"‚è±Ô∏è Time budget per framework: {time_budget} seconds\")\n",
    "    print(f\"üöÄ AutoML Performance Mode: {'ON' if BENCHMARK_CONFIG.get('enable_automl_optimization', False) else 'OFF'}\")\n",
    "    print(f\"‚ö° Advanced Optimization: {'HYPERX' if BENCHMARK_CONFIG.get('optimization_strategy') == 'hyperx' else 'Standard'}\")\n",
    "    print(f\"üîß Ensemble Methods: {'Enabled' if BENCHMARK_CONFIG.get('enable_ensemble', False) else 'Disabled'}\")\n",
    "    print(f\"üéØ Feature Selection: {'Automatic' if BENCHMARK_CONFIG.get('enable_feature_selection', False) else 'Manual'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_runs = len(selected_datasets) * len(selected_frameworks)\n",
    "    current_run = 0\n",
    "    \n",
    "    # Progress tracking\n",
    "    if WIDGETS_AVAILABLE:\n",
    "        progress = IntProgress(min=0, max=total_runs, description='Progress:')\n",
    "        display(progress)\n",
    "    \n",
    "    benchmark_results = []  # Reset results\n",
    "    \n",
    "    for dataset_name in selected_datasets:\n",
    "        print(f\"\\nüìä Testing dataset: {dataset_name}\")\n",
    "        \n",
    "        for framework_name in selected_frameworks:\n",
    "            current_run += 1\n",
    "            \n",
    "            if WIDGETS_AVAILABLE:\n",
    "                progress.value = current_run\n",
    "                progress.description = f'Running {framework_name} on {dataset_name}'\n",
    "            \n",
    "            print(f\"  ü§ñ Framework {current_run}/{total_runs}: {framework_name}\")\n",
    "            \n",
    "            # Get framework function\n",
    "            framework_func, available = AVAILABLE_FRAMEWORKS[framework_name]\n",
    "            \n",
    "            if not available:\n",
    "                print(f\"    ‚ùå {framework_name} not available, skipping\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Run benchmark\n",
    "                start_time = time.time()\n",
    "                result = benchmark_framework(framework_func, dataset_name, framework_name)\n",
    "                duration = time.time() - start_time\n",
    "                \n",
    "                benchmark_results.append(result)\n",
    "                \n",
    "                if result.success:\n",
    "                    print(f\"    ‚úÖ Completed in {duration:.2f}s - Score: {result.test_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"    ‚ùå Failed: {result.error_message}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if WIDGETS_AVAILABLE:\n",
    "        progress.description = 'Completed!'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üèÅ Benchmark completed!\")\n",
    "    print(f\"üìä Total results: {len(benchmark_results)}\")\n",
    "    successful_results = [r for r in benchmark_results if r.success]\n",
    "    print(f\"‚úÖ Successful runs: {len(successful_results)}\")\n",
    "    print(f\"‚ùå Failed runs: {len(benchmark_results) - len(successful_results)}\")\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# Button to run benchmark\n",
    "if WIDGETS_AVAILABLE:\n",
    "    run_button = widgets.Button(\n",
    "        description='üöÄ Run Benchmark',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='200px', height='40px')\n",
    "    )\n",
    "    \n",
    "    def on_run_clicked(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            run_comprehensive_benchmark()\n",
    "    \n",
    "    run_button.on_click(on_run_clicked)\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    display(widgets.VBox([run_button, output]))\n",
    "else:\n",
    "    print(\"üìù To run benchmark, execute: run_comprehensive_benchmark()\")\n",
    "\n",
    "print(\"üéØ Benchmark runner ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba7229",
   "metadata": {},
   "source": [
    "# 8. Compare Results and Visualization\n",
    "\n",
    "Let's analyze and visualize the benchmark results to compare the performance of different AutoML frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9aa386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2594adc3e5ee48f7b6366dac8342aac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='üìä Analyze Results', layout=Layout(height='40px', wi‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cdc026336c49eab3027eb4b29b9bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analysis and visualization functions ready!\n"
     ]
    }
   ],
   "source": [
    "def analyze_results():\n",
    "    \"\"\"Analyze and display benchmark results.\"\"\"\n",
    "    if not benchmark_results:\n",
    "        print(\"‚ùå No benchmark results available. Please run the benchmark first.\")\n",
    "        return\n",
    "    \n",
    "    successful_results = [r for r in benchmark_results if r.success]\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"‚ùå No successful benchmark results to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_data = []\n",
    "    for result in successful_results:\n",
    "        results_data.append({\n",
    "            'Framework': result.approach,\n",
    "            'Dataset': result.dataset_name,\n",
    "            'Task Type': result.task_type,\n",
    "            'Test Score': result.test_score,\n",
    "            'Training Time (s)': result.training_time,\n",
    "            'Memory Peak (MB)': result.memory_peak_mb,\n",
    "            'Dataset Size': f\"{result.dataset_size[0]}√ó{result.dataset_size[1]}\",\n",
    "            'CV Score Mean': result.cv_score_mean,\n",
    "            'CV Score Std': result.cv_score_std\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    \n",
    "    print(f\"üìä Analysis Summary:\")\n",
    "    print(f\"  ‚Ä¢ Total successful runs: {len(successful_results)}\")\n",
    "    print(f\"  ‚Ä¢ Frameworks tested: {df_results['Framework'].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Datasets tested: {df_results['Dataset'].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Classification tasks: {len(df_results[df_results['Task Type'] == 'classification'])}\")\n",
    "    print(f\"  ‚Ä¢ Regression tasks: {len(df_results[df_results['Task Type'] == 'regression'])}\")\n",
    "    \n",
    "    # Display detailed results table\n",
    "    print(\"\\nüìã Detailed Results:\")\n",
    "    styled_df = df_results.style.format({\n",
    "        'Test Score': '{:.4f}',\n",
    "        'Training Time (s)': '{:.2f}',\n",
    "        'Memory Peak (MB)': '{:.1f}',\n",
    "        'CV Score Mean': '{:.4f}',\n",
    "        'CV Score Std': '{:.4f}'\n",
    "    }).background_gradient(subset=['Test Score'], cmap='RdYlGn')\n",
    "    \n",
    "    display(styled_df)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "def create_comparison_visualizations(df_results=None):\n",
    "    \"\"\"Create comprehensive comparison visualizations.\"\"\"\n",
    "    if df_results is None:\n",
    "        df_results = analyze_results()\n",
    "    \n",
    "    if df_results is None or df_results.empty:\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting area\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Test Score Comparison by Framework\n",
    "    plt.subplot(3, 3, 1)\n",
    "    sns.boxplot(data=df_results, x='Framework', y='Test Score')\n",
    "    plt.title('Test Score by Framework', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Test Score')\n",
    "    \n",
    "    # 2. Training Time Comparison\n",
    "    plt.subplot(3, 3, 2)\n",
    "    sns.boxplot(data=df_results, x='Framework', y='Training Time (s)')\n",
    "    plt.title('Training Time by Framework', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    \n",
    "    # 3. Memory Usage Comparison\n",
    "    plt.subplot(3, 3, 3)\n",
    "    sns.boxplot(data=df_results, x='Framework', y='Memory Peak (MB)')\n",
    "    plt.title('Memory Usage by Framework', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Peak Memory (MB)')\n",
    "    \n",
    "    # 4. Performance vs Training Time\n",
    "    plt.subplot(3, 3, 4)\n",
    "    for framework in df_results['Framework'].unique():\n",
    "        fw_data = df_results[df_results['Framework'] == framework]\n",
    "        plt.scatter(fw_data['Training Time (s)'], fw_data['Test Score'], \n",
    "                   label=framework, alpha=0.7, s=60)\n",
    "    plt.xlabel('Training Time (seconds)')\n",
    "    plt.ylabel('Test Score')\n",
    "    plt.title('Performance vs Training Time', fontsize=14, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 5. Performance by Dataset\n",
    "    plt.subplot(3, 3, 5)\n",
    "    sns.boxplot(data=df_results, x='Dataset', y='Test Score')\n",
    "    plt.title('Performance by Dataset', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Test Score')\n",
    "    \n",
    "    # 6. Heatmap of Framework vs Dataset performance\n",
    "    plt.subplot(3, 3, 6)\n",
    "    pivot_df = df_results.pivot_table(values='Test Score', index='Framework', columns='Dataset', aggfunc='mean')\n",
    "    sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='RdYlGn', cbar_kws={'label': 'Test Score'})\n",
    "    plt.title('Framework vs Dataset Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 7. CV Score Mean vs Std\n",
    "    plt.subplot(3, 3, 7)\n",
    "    for framework in df_results['Framework'].unique():\n",
    "        fw_data = df_results[df_results['Framework'] == framework]\n",
    "        plt.scatter(fw_data['CV Score Std'], fw_data['CV Score Mean'], \n",
    "                   label=framework, alpha=0.7, s=60)\n",
    "    plt.xlabel('CV Score Standard Deviation')\n",
    "    plt.ylabel('CV Score Mean')\n",
    "    plt.title('Cross-Validation Consistency', fontsize=14, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 8. Training Time by Task Type\n",
    "    plt.subplot(3, 3, 8)\n",
    "    sns.boxplot(data=df_results, x='Task Type', y='Training Time (s)', hue='Framework')\n",
    "    plt.title('Training Time by Task Type', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 9. Framework Ranking\n",
    "    plt.subplot(3, 3, 9)\n",
    "    framework_stats = df_results.groupby('Framework').agg({\n",
    "        'Test Score': 'mean',\n",
    "        'Training Time (s)': 'mean',\n",
    "        'Memory Peak (MB)': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Normalize scores for ranking (higher is better for test score, lower is better for time/memory)\n",
    "    framework_stats['Score_norm'] = (framework_stats['Test Score'] - framework_stats['Test Score'].min()) / (framework_stats['Test Score'].max() - framework_stats['Test Score'].min())\n",
    "    framework_stats['Time_norm'] = 1 - (framework_stats['Training Time (s)'] - framework_stats['Training Time (s)'].min()) / (framework_stats['Training Time (s)'].max() - framework_stats['Training Time (s)'].min())\n",
    "    framework_stats['Memory_norm'] = 1 - (framework_stats['Memory Peak (MB)'] - framework_stats['Memory Peak (MB)'].min()) / (framework_stats['Memory Peak (MB)'].max() - framework_stats['Memory Peak (MB)'].min())\n",
    "    \n",
    "    # Composite score (equal weights)\n",
    "    framework_stats['Composite_Score'] = (framework_stats['Score_norm'] + framework_stats['Time_norm'] + framework_stats['Memory_norm']) / 3\n",
    "    framework_stats_sorted = framework_stats.sort_values('Composite_Score', ascending=False)\n",
    "    \n",
    "    plt.barh(range(len(framework_stats_sorted)), framework_stats_sorted['Composite_Score'])\n",
    "    plt.yticks(range(len(framework_stats_sorted)), framework_stats_sorted.index)\n",
    "    plt.xlabel('Composite Score (Normalized)')\n",
    "    plt.title('Overall Framework Ranking', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display ranking table\n",
    "    print(\"\\nüèÜ Framework Ranking (Composite Score):\")\n",
    "    ranking_display = framework_stats_sorted[['Test Score', 'Training Time (s)', 'Memory Peak (MB)', 'Composite_Score']].round(4)\n",
    "    display(ranking_display.style.background_gradient(subset=['Composite_Score'], cmap='RdYlGn'))\n",
    "    \n",
    "    return framework_stats_sorted\n",
    "\n",
    "# Interactive button to analyze results\n",
    "if WIDGETS_AVAILABLE:\n",
    "    analyze_button = widgets.Button(\n",
    "        description='üìä Analyze Results',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='200px', height='40px')\n",
    "    )\n",
    "    \n",
    "    visualize_button = widgets.Button(\n",
    "        description='üìà Create Visualizations',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='200px', height='40px')\n",
    "    )\n",
    "    \n",
    "    def on_analyze_clicked(b):\n",
    "        with analysis_output:\n",
    "            clear_output(wait=True)\n",
    "            df_results = analyze_results()\n",
    "    \n",
    "    def on_visualize_clicked(b):\n",
    "        with analysis_output:\n",
    "            clear_output(wait=True)\n",
    "            df_results = analyze_results()\n",
    "            if df_results is not None:\n",
    "                create_comparison_visualizations(df_results)\n",
    "    \n",
    "    analyze_button.on_click(on_analyze_clicked)\n",
    "    visualize_button.on_click(on_visualize_clicked)\n",
    "    \n",
    "    analysis_output = widgets.Output()\n",
    "    \n",
    "    display(widgets.HBox([analyze_button, visualize_button]))\n",
    "    display(analysis_output)\n",
    "else:\n",
    "    print(\"üìù To analyze results, execute: analyze_results()\")\n",
    "    print(\"üìù To create visualizations, execute: create_comparison_visualizations()\")\n",
    "\n",
    "print(\"üìä Analysis and visualization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9015120",
   "metadata": {},
   "source": [
    "# 9. Statistical Analysis of Results\n",
    "\n",
    "Let's perform statistical tests to determine if there are significant differences between the AutoML frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0e8e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae8eb3cc70a44a99d7159d970c0809b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='warning', description='üìä Statistical Analysis', layout=Layout(height='40px‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e343938ef61f49a5b4961aa1ec87ca8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Statistical analysis functions ready!\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "def perform_statistical_analysis(df_results=None):\n",
    "    \"\"\"Perform statistical analysis on benchmark results.\"\"\"\n",
    "    if df_results is None:\n",
    "        df_results = analyze_results()\n",
    "    \n",
    "    if df_results is None or df_results.empty:\n",
    "        print(\"‚ùå No results available for statistical analysis.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Statistical Analysis of AutoML Framework Performance\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Descriptive Statistics\n",
    "    print(\"\\n1Ô∏è‚É£ Descriptive Statistics by Framework:\")\n",
    "    desc_stats = df_results.groupby('Framework')[['Test Score', 'Training Time (s)', 'Memory Peak (MB)']].describe().round(4)\n",
    "    display(desc_stats)\n",
    "    \n",
    "    # 2. ANOVA Test for Test Scores\n",
    "    print(\"\\n2Ô∏è‚É£ ANOVA Test for Test Score Differences:\")\n",
    "    frameworks = df_results['Framework'].unique()\n",
    "    if len(frameworks) >= 2:\n",
    "        test_scores_by_framework = [df_results[df_results['Framework'] == fw]['Test Score'].values for fw in frameworks]\n",
    "        \n",
    "        # Remove empty groups\n",
    "        test_scores_by_framework = [scores for scores in test_scores_by_framework if len(scores) > 0]\n",
    "        \n",
    "        if len(test_scores_by_framework) >= 2 and all(len(scores) > 0 for scores in test_scores_by_framework):\n",
    "            f_statistic, p_value = stats.f_oneway(*test_scores_by_framework)\n",
    "            print(f\"  F-statistic: {f_statistic:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(\"  ‚úÖ Significant differences found between frameworks (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"  ‚ùå No significant differences found between frameworks (p >= 0.05)\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Insufficient data for ANOVA test\")\n",
    "    \n",
    "    # 3. Pairwise t-tests for Test Scores\n",
    "    print(\"\\n3Ô∏è‚É£ Pairwise t-tests for Test Score Differences:\")\n",
    "    if len(frameworks) >= 2:\n",
    "        pairwise_results = []\n",
    "        \n",
    "        for fw1, fw2 in combinations(frameworks, 2):\n",
    "            scores1 = df_results[df_results['Framework'] == fw1]['Test Score'].values\n",
    "            scores2 = df_results[df_results['Framework'] == fw2]['Test Score'].values\n",
    "            \n",
    "            if len(scores1) > 0 and len(scores2) > 0:\n",
    "                try:\n",
    "                    t_stat, p_val = stats.ttest_ind(scores1, scores2)\n",
    "                    mean_diff = np.mean(scores1) - np.mean(scores2)\n",
    "                    \n",
    "                    pairwise_results.append({\n",
    "                        'Framework 1': fw1,\n",
    "                        'Framework 2': fw2,\n",
    "                        'Mean Diff': mean_diff,\n",
    "                        't-statistic': t_stat,\n",
    "                        'p-value': p_val,\n",
    "                        'Significant': 'Yes' if p_val < 0.05 else 'No'\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if pairwise_results:\n",
    "            pairwise_df = pd.DataFrame(pairwise_results)\n",
    "            pairwise_df = pairwise_df.round(4)\n",
    "            display(pairwise_df.style.apply(lambda x: ['background-color: lightgreen' if v == 'Yes' else '' for v in x], subset=['Significant']))\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è No valid pairwise comparisons possible\")\n",
    "    \n",
    "    # 4. Correlation Analysis\n",
    "    print(\"\\n4Ô∏è‚É£ Correlation Analysis:\")\n",
    "    numeric_cols = ['Test Score', 'Training Time (s)', 'Memory Peak (MB)', 'CV Score Mean']\n",
    "    correlation_matrix = df_results[numeric_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    plt.title('Correlation Matrix of Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Performance Consistency Analysis\n",
    "    print(\"\\n5Ô∏è‚É£ Performance Consistency Analysis:\")\n",
    "    consistency_stats = df_results.groupby('Framework').agg({\n",
    "        'Test Score': ['mean', 'std', 'min', 'max'],\n",
    "        'Training Time (s)': ['mean', 'std'],\n",
    "        'Memory Peak (MB)': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    # Calculate coefficient of variation for consistency measure\n",
    "    cv_scores = df_results.groupby('Framework')['Test Score'].apply(lambda x: x.std() / x.mean() if x.mean() != 0 else np.inf)\n",
    "    cv_time = df_results.groupby('Framework')['Training Time (s)'].apply(lambda x: x.std() / x.mean() if x.mean() != 0 else np.inf)\n",
    "    \n",
    "    consistency_df = pd.DataFrame({\n",
    "        'Score CV': cv_scores,\n",
    "        'Time CV': cv_time\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n  Coefficient of Variation (lower = more consistent):\")\n",
    "    display(consistency_df.style.background_gradient(cmap='RdYlGn_r'))\n",
    "    \n",
    "    # 6. Effect Size Analysis (Cohen's d)\n",
    "    print(\"\\n6Ô∏è‚É£ Effect Size Analysis (Cohen's d):\")\n",
    "    if len(frameworks) >= 2:\n",
    "        effect_sizes = []\n",
    "        \n",
    "        for fw1, fw2 in combinations(frameworks, 2):\n",
    "            scores1 = df_results[df_results['Framework'] == fw1]['Test Score'].values\n",
    "            scores2 = df_results[df_results['Framework'] == fw2]['Test Score'].values\n",
    "            \n",
    "            if len(scores1) > 1 and len(scores2) > 1:\n",
    "                # Calculate Cohen's d\n",
    "                mean1, mean2 = np.mean(scores1), np.mean(scores2)\n",
    "                std1, std2 = np.std(scores1, ddof=1), np.std(scores2, ddof=1)\n",
    "                pooled_std = np.sqrt(((len(scores1) - 1) * std1**2 + (len(scores2) - 1) * std2**2) / \n",
    "                                   (len(scores1) + len(scores2) - 2))\n",
    "                \n",
    "                cohens_d = (mean1 - mean2) / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                # Interpret effect size\n",
    "                if abs(cohens_d) < 0.2:\n",
    "                    interpretation = \"Negligible\"\n",
    "                elif abs(cohens_d) < 0.5:\n",
    "                    interpretation = \"Small\"\n",
    "                elif abs(cohens_d) < 0.8:\n",
    "                    interpretation = \"Medium\"\n",
    "                else:\n",
    "                    interpretation = \"Large\"\n",
    "                \n",
    "                effect_sizes.append({\n",
    "                    'Framework 1': fw1,\n",
    "                    'Framework 2': fw2,\n",
    "                    \"Cohen's d\": cohens_d,\n",
    "                    'Effect Size': interpretation\n",
    "                })\n",
    "        \n",
    "        if effect_sizes:\n",
    "            effect_df = pd.DataFrame(effect_sizes)\n",
    "            effect_df = effect_df.round(4)\n",
    "            display(effect_df)\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Insufficient data for effect size analysis\")\n",
    "    \n",
    "    print(\"\\nüìä Statistical analysis completed!\")\n",
    "    return df_results\n",
    "\n",
    "def generate_comprehensive_report():\n",
    "    \"\"\"Generate a comprehensive comparison report.\"\"\"\n",
    "    if not benchmark_results:\n",
    "        print(\"‚ùå No benchmark results available. Please run the benchmark first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìÑ Generating Comprehensive AutoML Comparison Report...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Analyze results\n",
    "    df_results = analyze_results()\n",
    "    \n",
    "    if df_results is None:\n",
    "        return\n",
    "    \n",
    "    # Create summary\n",
    "    successful_results = [r for r in benchmark_results if r.success]\n",
    "    failed_results = [r for r in benchmark_results if not r.success]\n",
    "    \n",
    "    print(f\"\\nüìä Executive Summary:\")\n",
    "    print(f\"  ‚Ä¢ Total benchmark runs: {len(benchmark_results)}\")\n",
    "    print(f\"  ‚Ä¢ Successful runs: {len(successful_results)}\")\n",
    "    print(f\"  ‚Ä¢ Failed runs: {len(failed_results)}\")\n",
    "    print(f\"  ‚Ä¢ Frameworks tested: {df_results['Framework'].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Datasets tested: {df_results['Dataset'].nunique()}\")\n",
    "    \n",
    "    # Framework availability\n",
    "    print(f\"\\nü§ñ Framework Availability:\")\n",
    "    for name, (func, available) in AVAILABLE_FRAMEWORKS.items():\n",
    "        status = \"‚úÖ Available\" if available else \"‚ùå Not Available\"\n",
    "        print(f\"  ‚Ä¢ {name}: {status}\")\n",
    "    \n",
    "    # Best performing framework\n",
    "    best_framework = df_results.groupby('Framework')['Test Score'].mean().idxmax()\n",
    "    best_score = df_results.groupby('Framework')['Test Score'].mean().max()\n",
    "    print(f\"\\nüèÜ Best Overall Performance: {best_framework} ({best_score:.4f} average score)\")\n",
    "    \n",
    "    # Fastest framework\n",
    "    fastest_framework = df_results.groupby('Framework')['Training Time (s)'].mean().idxmin()\n",
    "    fastest_time = df_results.groupby('Framework')['Training Time (s)'].mean().min()\n",
    "    print(f\"‚ö° Fastest Training: {fastest_framework} ({fastest_time:.2f}s average)\")\n",
    "    \n",
    "    # Most memory efficient\n",
    "    most_efficient = df_results.groupby('Framework')['Memory Peak (MB)'].mean().idxmin()\n",
    "    lowest_memory = df_results.groupby('Framework')['Memory Peak (MB)'].mean().min()\n",
    "    print(f\"üíæ Most Memory Efficient: {most_efficient} ({lowest_memory:.1f}MB average)\")\n",
    "    \n",
    "    # Failed runs analysis\n",
    "    if failed_results:\n",
    "        print(f\"\\n‚ùå Failed Runs Analysis:\")\n",
    "        failure_counts = {}\n",
    "        for result in failed_results:\n",
    "            if result.approach not in failure_counts:\n",
    "                failure_counts[result.approach] = 0\n",
    "            failure_counts[result.approach] += 1\n",
    "        \n",
    "        for framework, count in failure_counts.items():\n",
    "            print(f\"  ‚Ä¢ {framework}: {count} failures\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    print(f\"  ‚Ä¢ For best accuracy: Use {best_framework}\")\n",
    "    print(f\"  ‚Ä¢ For fastest training: Use {fastest_framework}\")\n",
    "    print(f\"  ‚Ä¢ For memory efficiency: Use {most_efficient}\")\n",
    "    \n",
    "    if KOLOSAL_AVAILABLE and 'kolosal' in df_results['Framework'].values:\n",
    "        kolosal_stats = df_results[df_results['Framework'] == 'kolosal']\n",
    "        avg_score = kolosal_stats['Test Score'].mean()\n",
    "        avg_time = kolosal_stats['Training Time (s)'].mean()\n",
    "        print(f\"\\nüèóÔ∏è Kolosal AutoML Performance:\")\n",
    "        print(f\"  ‚Ä¢ Average Test Score: {avg_score:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Average Training Time: {avg_time:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Successful runs: {len(kolosal_stats)}\")\n",
    "    \n",
    "    print(\"\\nüìÑ Report generation completed!\")\n",
    "\n",
    "# Statistical analysis button\n",
    "if WIDGETS_AVAILABLE:\n",
    "    stats_button = widgets.Button(\n",
    "        description='üìä Statistical Analysis',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='200px', height='40px')\n",
    "    )\n",
    "    \n",
    "    report_button = widgets.Button(\n",
    "        description='üìÑ Generate Report',\n",
    "        button_style='danger',\n",
    "        layout=widgets.Layout(width='200px', height='40px')\n",
    "    )\n",
    "    \n",
    "    def on_stats_clicked(b):\n",
    "        with stats_output:\n",
    "            clear_output(wait=True)\n",
    "            perform_statistical_analysis()\n",
    "    \n",
    "    def on_report_clicked(b):\n",
    "        with stats_output:\n",
    "            clear_output(wait=True)\n",
    "            generate_comprehensive_report()\n",
    "    \n",
    "    stats_button.on_click(on_stats_clicked)\n",
    "    report_button.on_click(on_report_clicked)\n",
    "    \n",
    "    stats_output = widgets.Output()\n",
    "    \n",
    "    display(widgets.HBox([stats_button, report_button]))\n",
    "    display(stats_output)\n",
    "else:\n",
    "    print(\"üìù To perform statistical analysis, execute: perform_statistical_analysis()\")\n",
    "    print(\"üìù To generate comprehensive report, execute: generate_comprehensive_report()\")\n",
    "\n",
    "print(\"üìä Statistical analysis functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30bc7d",
   "metadata": {},
   "source": [
    "# üéØ Full Experiment - Comprehensive AutoML Comparison\n",
    "\n",
    "This section runs a comprehensive comparison experiment across multiple AutoML frameworks and datasets. The experiment includes:\n",
    "\n",
    "## üîß Experiment Features:\n",
    "- **Multiple Datasets**: Small, medium, and synthetic datasets for comprehensive testing\n",
    "- **All Available Frameworks**: Tests all installed AutoML platforms\n",
    "- **Configurable Parameters**: Easily adjust time budgets, dataset selection, and framework limits\n",
    "- **Automatic Analysis**: Generates statistical analysis, visualizations, and comprehensive reports\n",
    "- **Performance Metrics**: Accuracy, training time, memory usage, and consistency analysis\n",
    "- **Kolosal-Specific Analysis**: Detailed performance analysis of Kolosal AutoML\n",
    "\n",
    "## ‚öôÔ∏è Configuration Options:\n",
    "- `datasets`: List of datasets to test\n",
    "- `time_budget`: Time limit per framework (in seconds)\n",
    "- `include_large_datasets`: Whether to include computationally expensive datasets\n",
    "- `max_frameworks`: Limit the number of frameworks to test\n",
    "\n",
    "## üìä What You'll Get:\n",
    "1. **Performance Rankings**: Best frameworks by accuracy, speed, and memory efficiency\n",
    "2. **Statistical Analysis**: ANOVA tests, effect sizes, and significance testing\n",
    "3. **Comprehensive Visualizations**: Comparison charts, heatmaps, and scatter plots\n",
    "4. **Detailed Report**: Executive summary with recommendations\n",
    "5. **Kolosal AutoML Analysis**: Specific performance insights for our framework\n",
    "\n",
    "## üöÄ Ready to Run:\n",
    "Execute the cell below to start the full experiment. You can modify the configuration parameters before running to customize the experiment for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7f9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Full AutoML Platforms Comparison Experiment...\n",
      "======================================================================\n",
      "üìä Experiment Configuration:\n",
      "  ‚Ä¢ Datasets: 7 datasets\n",
      "  ‚Ä¢ Frameworks: 5 frameworks\n",
      "  ‚Ä¢ Time budget per framework: 180 seconds\n",
      "  ‚Ä¢ Total estimated time: 105.0 minutes\n",
      "\n",
      "ü§ñ Frameworks to test: Standard ML, Kolosal AutoML, FLAML, AutoGluon, MLjar-Supervised\n",
      "\n",
      "üìä Datasets to test: iris, wine, breast_cancer, digits, diabetes, synthetic_small_classification, synthetic_small_regression\n",
      "\n",
      "‚ö†Ô∏è This is a comprehensive experiment that will take significant time.\n",
      "üí° You can modify FULL_EXP_CONFIG above to customize the experiment.\n",
      "\n",
      "üöÄ Starting benchmark execution...\n",
      "üöÄ Starting comprehensive benchmark...\n",
      "üìä Datasets: ['iris', 'wine', 'breast_cancer', 'digits', 'diabetes', 'synthetic_small_classification', 'synthetic_small_regression']\n",
      "ü§ñ Frameworks: ['Standard ML', 'Kolosal AutoML', 'FLAML', 'AutoGluon', 'MLjar-Supervised']\n",
      "‚è±Ô∏è Time budget per framework: 180 seconds\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9295fd4fea4b96b9d17a71e586e42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Progress:', max=35)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Testing dataset: iris\n",
      "  ü§ñ Framework 1/35: Standard ML\n",
      "üìä Loading dataset: iris\n",
      "üöÄ Running Standard ML on iris...\n",
      "üöÄ Running Standard ML on iris...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 20:03:17,410 - INFO - MLTrainingEngine - Data preprocessor skipped for fast initialization\n",
      "2025-08-06 20:03:17,411 - INFO - kolosal_automl.modules.engine.batch_processor - Memory-aware batch processing enabled\n",
      "2025-08-06 20:03:17,412 - INFO - MLTrainingEngine - Batch processor initialized\n",
      "2025-08-06 20:03:17,415 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Advanced resource management configured with 4 threads\n",
      "2025-08-06 20:03:17,411 - INFO - kolosal_automl.modules.engine.batch_processor - Memory-aware batch processing enabled\n",
      "2025-08-06 20:03:17,412 - INFO - MLTrainingEngine - Batch processor initialized\n",
      "2025-08-06 20:03:17,415 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Advanced resource management configured with 4 threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Standard ML completed: 0.9000 score in 0.46s\n",
      "    ‚úÖ Completed in 0.46s - Score: 0.9000\n",
      "  ü§ñ Framework 2/35: Kolosal AutoML\n",
      "üìä Loading dataset: iris\n",
      "üöÄ Running Kolosal AutoML on iris...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 20:03:18,739 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - SIMD optimizer initialized\n",
      "2025-08-06 20:03:18,741 - INFO - kolosal_automl.modules.engine.quantizer - Quantizer initialized with QuantizationType.INT8 type and QuantizationMode.DYNAMIC mode\n",
      "2025-08-06 20:03:18,754 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Quantizer initialized with QuantizationType.INT8 type\n",
      "2025-08-06 20:03:18,756 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Result cache initialized with 1000 entries\n",
      "2025-08-06 20:03:18,741 - INFO - kolosal_automl.modules.engine.quantizer - Quantizer initialized with QuantizationType.INT8 type and QuantizationMode.DYNAMIC mode\n",
      "2025-08-06 20:03:18,754 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Quantizer initialized with QuantizationType.INT8 type\n",
      "2025-08-06 20:03:18,756 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Result cache initialized with 1000 entries\n",
      "2025-08-06 20:03:18,812 - INFO - kolosal_automl.modules.engine.jit_compiler - JIT compiler initialized with Numba support\n",
      "2025-08-06 20:03:18,814 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - JIT compiler enabled for inference optimization\n",
      "2025-08-06 20:03:18,812 - INFO - kolosal_automl.modules.engine.jit_compiler - JIT compiler initialized with Numba support\n",
      "2025-08-06 20:03:18,814 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - JIT compiler enabled for inference optimization\n",
      "2025-08-06 20:03:20,141 - INFO - kolosal_automl.modules.engine.mixed_precision - PyTorch AMP scaler initialized\n",
      "2025-08-06 20:03:20,142 - INFO - kolosal_automl.modules.engine.mixed_precision - Mixed precision enabled\n",
      "2025-08-06 20:03:20,144 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Mixed precision enabled for inference\n",
      "2025-08-06 20:03:20,144 - INFO - kolosal_automl.modules.engine.streaming_pipeline - Streaming pipeline initialized with chunk_size=1000, parallel=True, distributed=False\n",
      "2025-08-06 20:03:20,145 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Streaming pipeline available for batch inference\n",
      "2025-08-06 20:03:20,147 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Resource monitoring started\n",
      "2025-08-06 20:03:20,149 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Inference engine initialized with model version 1.0\n",
      "2025-08-06 20:03:20,151 - INFO - MLTrainingEngine - Inference engine initialized\n",
      "2025-08-06 20:03:20,152 - INFO - MLTrainingEngine - Model monitoring initialized\n",
      "2025-08-06 20:03:20,141 - INFO - kolosal_automl.modules.engine.mixed_precision - PyTorch AMP scaler initialized\n",
      "2025-08-06 20:03:20,142 - INFO - kolosal_automl.modules.engine.mixed_precision - Mixed precision enabled\n",
      "2025-08-06 20:03:20,144 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Mixed precision enabled for inference\n",
      "2025-08-06 20:03:20,144 - INFO - kolosal_automl.modules.engine.streaming_pipeline - Streaming pipeline initialized with chunk_size=1000, parallel=True, distributed=False\n",
      "2025-08-06 20:03:20,145 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Streaming pipeline available for batch inference\n",
      "2025-08-06 20:03:20,147 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Resource monitoring started\n",
      "2025-08-06 20:03:20,149 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Inference engine initialized with model version 1.0\n",
      "2025-08-06 20:03:20,151 - INFO - MLTrainingEngine - Inference engine initialized\n",
      "2025-08-06 20:03:20,152 - INFO - MLTrainingEngine - Model monitoring initialized\n",
      "2025-08-06 20:03:20,154 - INFO - MLTrainingEngine - Optimization components disabled for fast initialization\n",
      "2025-08-06 20:03:20,155 - INFO - MLTrainingEngine - ML Training Engine v0.1.4 initialized with task type: TaskType.CLASSIFICATION\n",
      "2025-08-06 20:03:20,156 - INFO - MLTrainingEngine - GPU usage enabled with memory fraction: 0.8\n",
      "2025-08-06 20:03:20,154 - INFO - MLTrainingEngine - Optimization components disabled for fast initialization\n",
      "2025-08-06 20:03:20,155 - INFO - MLTrainingEngine - ML Training Engine v0.1.4 initialized with task type: TaskType.CLASSIFICATION\n",
      "2025-08-06 20:03:20,156 - INFO - MLTrainingEngine - GPU usage enabled with memory fraction: 0.8\n",
      "2025-08-06 20:03:20,157 - INFO - MLTrainingEngine - Model types registered for auto discovery\n",
      "2025-08-06 20:03:20,422 - INFO - MLTrainingEngine - Using custom model: RandomForestClassifier\n",
      "2025-08-06 20:03:20,427 - INFO - MLTrainingEngine - Test data split: 24 test samples reserved for final evaluation\n",
      "2025-08-06 20:03:20,429 - INFO - MLTrainingEngine - Using provided validation data: 30 validation samples, 24 test samples reserved\n",
      "2025-08-06 20:03:20,157 - INFO - MLTrainingEngine - Model types registered for auto discovery\n",
      "2025-08-06 20:03:20,422 - INFO - MLTrainingEngine - Using custom model: RandomForestClassifier\n",
      "2025-08-06 20:03:20,427 - INFO - MLTrainingEngine - Test data split: 24 test samples reserved for final evaluation\n",
      "2025-08-06 20:03:20,429 - INFO - MLTrainingEngine - Using provided validation data: 30 validation samples, 24 test samples reserved\n",
      "2025-08-06 20:03:20,927 - INFO - MLTrainingEngine - Starting hyperparameter optimization with OptimizationStrategy.RANDOM_SEARCH\n",
      "2025-08-06 20:03:20,927 - INFO - MLTrainingEngine - Starting hyperparameter optimization with OptimizationStrategy.RANDOM_SEARCH\n",
      "2025-08-06 20:03:40,362 - INFO - MLTrainingEngine - Best CV score: 0.9528 ¬± 0.0150\n",
      "2025-08-06 20:03:40,364 - INFO - MLTrainingEngine - Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_depth': 10}\n",
      "2025-08-06 20:03:40,362 - INFO - MLTrainingEngine - Best CV score: 0.9528 ¬± 0.0150\n",
      "2025-08-06 20:03:40,364 - INFO - MLTrainingEngine - Best parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_depth': 10}\n",
      "2025-08-06 20:03:40,543 - INFO - MLTrainingEngine - Evaluating model on validation data...\n",
      "2025-08-06 20:03:40,554 - INFO - MLTrainingEngine - Validation accuracy: 0.9333\n",
      "2025-08-06 20:03:40,543 - INFO - MLTrainingEngine - Evaluating model on validation data...\n",
      "2025-08-06 20:03:40,554 - INFO - MLTrainingEngine - Validation accuracy: 0.9333\n",
      "2025-08-06 20:03:40,556 - INFO - MLTrainingEngine - Validation f1: 0.9333\n",
      "2025-08-06 20:03:40,557 - INFO - MLTrainingEngine - Validation precision: 0.9333\n",
      "2025-08-06 20:03:40,558 - INFO - MLTrainingEngine - Validation recall: 0.9333\n",
      "2025-08-06 20:03:40,556 - INFO - MLTrainingEngine - Validation f1: 0.9333\n",
      "2025-08-06 20:03:40,557 - INFO - MLTrainingEngine - Validation precision: 0.9333\n",
      "2025-08-06 20:03:40,558 - INFO - MLTrainingEngine - Validation recall: 0.9333\n",
      "2025-08-06 20:03:40,758 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - WARNING - CPU usage (100.1%) above threshold, enabling throttling\n",
      "2025-08-06 20:03:40,768 - INFO - MLTrainingEngine - Top 10 features by importance:\n",
      "2025-08-06 20:03:40,770 - INFO - MLTrainingEngine -   feature_3: 0.4528\n",
      "2025-08-06 20:03:40,771 - INFO - MLTrainingEngine -   feature_2: 0.4125\n",
      "2025-08-06 20:03:40,772 - INFO - MLTrainingEngine -   feature_0: 0.1156\n",
      "2025-08-06 20:03:40,774 - INFO - MLTrainingEngine -   feature_1: 0.0191\n",
      "2025-08-06 20:03:40,758 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - WARNING - CPU usage (100.1%) above threshold, enabling throttling\n",
      "2025-08-06 20:03:40,768 - INFO - MLTrainingEngine - Top 10 features by importance:\n",
      "2025-08-06 20:03:40,770 - INFO - MLTrainingEngine -   feature_3: 0.4528\n",
      "2025-08-06 20:03:40,771 - INFO - MLTrainingEngine -   feature_2: 0.4125\n",
      "2025-08-06 20:03:40,772 - INFO - MLTrainingEngine -   feature_0: 0.1156\n",
      "2025-08-06 20:03:40,774 - INFO - MLTrainingEngine -   feature_1: 0.0191\n",
      "2025-08-06 20:03:40,977 - INFO - MLTrainingEngine - New best model: kolosal_model with score 0.9333\n",
      "2025-08-06 20:03:40,981 - INFO - MLTrainingEngine - Model 'kolosal_model' saved to ./benchmark_models\\kolosal_model.pkl\n",
      "2025-08-06 20:03:40,977 - INFO - MLTrainingEngine - New best model: kolosal_model with score 0.9333\n",
      "2025-08-06 20:03:40,981 - INFO - MLTrainingEngine - Model 'kolosal_model' saved to ./benchmark_models\\kolosal_model.pkl\n",
      "2025-08-06 20:03:40,982 - INFO - MLTrainingEngine - Model training completed in 20.82 seconds\n",
      "2025-08-06 20:03:40,982 - INFO - MLTrainingEngine - Model training completed in 20.82 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Kolosal AutoML failed: Training failed: Unknown training error\n",
      "    ‚ùå Failed: Training failed: Unknown training error\n",
      "  ü§ñ Framework 3/35: FLAML\n",
      "üìä Loading dataset: iris\n",
      "üöÄ Running FLAML on iris...\n",
      "üöÄ Running FLAML on iris...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 20:03:42,074 - INFO - flaml.tune.searcher.blendsearch - No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'. More info can be found at https://microsoft.github.io/FLAML/docs/FAQ#about-low_cost_partial_config-in-tune\n",
      "2025-08-06 20:03:50,894 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - CPU usage (28.7%) below threshold, disabling throttling\n",
      "2025-08-06 20:03:50,894 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - CPU usage (28.7%) below threshold, disabling throttling\n",
      "2025-08-06 20:04:11,117 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - WARNING - CPU usage (100.3%) above threshold, enabling throttling\n",
      "2025-08-06 20:04:11,117 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - WARNING - CPU usage (100.3%) above threshold, enabling throttling\n",
      "2025-08-06 20:05:47,165 - INFO - MLTrainingEngine - Received signal 2, cleaning up...\n",
      "2025-08-06 20:05:47,166 - INFO - MLTrainingEngine - Auto-saving best model before shutdown\n",
      "2025-08-06 20:05:47,168 - INFO - MLTrainingEngine - Model 'kolosal_model' saved to ./benchmark_models\\kolosal_model.pkl\n",
      "2025-08-06 20:05:47,169 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Shutting down inference engine\n",
      "2025-08-06 20:05:47,170 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Inference engine shutdown complete\n",
      "2025-08-06 20:05:47,170 - INFO - kolosal_automl.modules.engine.batch_processor - Stopping batch processor...\n",
      "2025-08-06 20:05:47,171 - INFO - kolosal_automl.modules.engine.batch_processor - Processed 0 remaining items during shutdown\n",
      "2025-08-06 20:05:47,165 - INFO - MLTrainingEngine - Received signal 2, cleaning up...\n",
      "2025-08-06 20:05:47,166 - INFO - MLTrainingEngine - Auto-saving best model before shutdown\n",
      "2025-08-06 20:05:47,168 - INFO - MLTrainingEngine - Model 'kolosal_model' saved to ./benchmark_models\\kolosal_model.pkl\n",
      "2025-08-06 20:05:47,169 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Shutting down inference engine\n",
      "2025-08-06 20:05:47,170 - kolosal_automl.modules.engine.inference_engine.InferenceEngine - INFO - Inference engine shutdown complete\n",
      "2025-08-06 20:05:47,170 - INFO - kolosal_automl.modules.engine.batch_processor - Stopping batch processor...\n",
      "2025-08-06 20:05:47,171 - INFO - kolosal_automl.modules.engine.batch_processor - Processed 0 remaining items during shutdown\n",
      "2025-08-06 20:05:47,810 - INFO - kolosal_automl.modules.engine.batch_processor - Batch processor stopped\n",
      "2025-08-06 20:05:47,812 - INFO - MLTrainingEngine - Cleanup complete\n",
      "2025-08-06 20:05:47,810 - INFO - kolosal_automl.modules.engine.batch_processor - Batch processor stopped\n",
      "2025-08-06 20:05:47,812 - INFO - MLTrainingEngine - Cleanup complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cleaning up logging: No module named 'modules'\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Full Experiment - Comprehensive AutoML Benchmark\n",
    "\n",
    "print(\"üöÄ Starting Full AutoML Platforms Comparison Experiment...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration for full experiment with AutoML Performance Optimization\n",
    "FULL_EXP_CONFIG = {\n",
    "    \"datasets\": [\n",
    "        # Small datasets for quick comparison\n",
    "        'iris', 'wine', 'breast_cancer', \n",
    "        # Medium datasets for performance testing\n",
    "        'digits', 'diabetes',\n",
    "        # Synthetic datasets for scalability\n",
    "        'synthetic_small_classification', 'synthetic_small_regression'\n",
    "    ],\n",
    "    \"time_budget\": 300,  # 5 minutes per framework (increased for AutoML optimization)\n",
    "    \"include_large_datasets\": True,  # Include larger datasets for performance testing\n",
    "    \"max_frameworks\": None,  # Set to number to limit frameworks (None = all available)\n",
    "    \n",
    "    # AutoML Performance Configuration\n",
    "    \"automl_performance_mode\": True,    # Enable performance-optimized AutoML\n",
    "    \"enable_advanced_optimization\": True,  # Enable advanced optimization strategies\n",
    "    \"parallel_processing\": True,        # Enable parallel processing where possible\n",
    "    \"memory_optimization\": True,        # Enable memory optimization\n",
    "    \"early_stopping\": True,            # Enable early stopping for efficiency\n",
    "    \n",
    "    # Advanced AutoML Settings\n",
    "    \"optimization_strategy\": \"hyperx\", # Use HYPERX optimization strategy\n",
    "    \"ensemble_methods\": True,          # Enable ensemble methods\n",
    "    \"feature_selection\": True,         # Enable automatic feature selection\n",
    "    \"adaptive_hyperopt\": True,         # Enable adaptive hyperparameter optimization\n",
    "}\n",
    "\n",
    "# Add large dataset if specified\n",
    "if FULL_EXP_CONFIG[\"include_large_datasets\"]:\n",
    "    FULL_EXP_CONFIG[\"datasets\"].extend(['california_housing', 'synthetic_medium_classification'])\n",
    "\n",
    "# Get all available frameworks\n",
    "available_frameworks_full = [name for name, (func, available) in AVAILABLE_FRAMEWORKS.items() if available]\n",
    "\n",
    "# Limit frameworks if specified\n",
    "if FULL_EXP_CONFIG[\"max_frameworks\"]:\n",
    "    available_frameworks_full = available_frameworks_full[:FULL_EXP_CONFIG[\"max_frameworks\"]]\n",
    "\n",
    "print(f\"üìä Experiment Configuration:\")\n",
    "print(f\"  ‚Ä¢ Datasets: {len(FULL_EXP_CONFIG['datasets'])} datasets\")\n",
    "print(f\"  ‚Ä¢ Frameworks: {len(available_frameworks_full)} frameworks\")\n",
    "print(f\"  ‚Ä¢ Time budget per framework: {FULL_EXP_CONFIG['time_budget']} seconds\")\n",
    "print(f\"  ‚Ä¢ Total estimated time: {len(FULL_EXP_CONFIG['datasets']) * len(available_frameworks_full) * FULL_EXP_CONFIG['time_budget'] / 60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nü§ñ Frameworks to test: {', '.join(available_frameworks_full)}\")\n",
    "print(f\"\\nüìä Datasets to test: {', '.join(FULL_EXP_CONFIG['datasets'])}\")\n",
    "\n",
    "# Confirm before running\n",
    "print(f\"\\n‚ö†Ô∏è This is a comprehensive experiment that will take significant time.\")\n",
    "print(f\"üí° You can modify FULL_EXP_CONFIG above to customize the experiment.\")\n",
    "\n",
    "# Run the full benchmark\n",
    "print(f\"\\nüöÄ Starting benchmark execution...\")\n",
    "try:\n",
    "    results = run_comprehensive_benchmark(\n",
    "        selected_datasets=FULL_EXP_CONFIG[\"datasets\"],\n",
    "        selected_frameworks=available_frameworks_full,\n",
    "        time_budget=FULL_EXP_CONFIG[\"time_budget\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Benchmark completed successfully!\")\n",
    "    print(f\"üìä Total results collected: {len(results)}\")\n",
    "    \n",
    "    # Immediate analysis\n",
    "    print(f\"\\nüìà Running analysis...\")\n",
    "    df_results = analyze_results()\n",
    "    \n",
    "    if df_results is not None and not df_results.empty:\n",
    "        print(f\"\\nüèÜ Quick Results Summary:\")\n",
    "        \n",
    "        # Best performer by metric\n",
    "        best_accuracy = df_results.groupby('Framework')['Test Score'].mean().idxmax()\n",
    "        best_accuracy_score = df_results.groupby('Framework')['Test Score'].mean().max()\n",
    "        print(f\"  ‚Ä¢ Best Accuracy: {best_accuracy} ({best_accuracy_score:.4f})\")\n",
    "        \n",
    "        fastest_framework = df_results.groupby('Framework')['Training Time (s)'].mean().idxmin()\n",
    "        fastest_time = df_results.groupby('Framework')['Training Time (s)'].mean().min()\n",
    "        print(f\"  ‚Ä¢ Fastest Training: {fastest_framework} ({fastest_time:.2f}s)\")\n",
    "        \n",
    "        most_efficient = df_results.groupby('Framework')['Memory Peak (MB)'].mean().idxmin()\n",
    "        lowest_memory = df_results.groupby('Framework')['Memory Peak (MB)'].mean().min()\n",
    "        print(f\"  ‚Ä¢ Most Memory Efficient: {most_efficient} ({lowest_memory:.1f}MB)\")\n",
    "        \n",
    "        # Dataset-specific results\n",
    "        print(f\"\\nüìä Results by Dataset:\")\n",
    "        dataset_summary = df_results.groupby('Dataset').agg({\n",
    "            'Test Score': ['mean', 'std', 'count'],\n",
    "            'Training Time (s)': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        for dataset in FULL_EXP_CONFIG['datasets']:\n",
    "            if dataset in df_results['Dataset'].values:\n",
    "                dataset_data = df_results[df_results['Dataset'] == dataset]\n",
    "                best_framework_for_dataset = dataset_data.loc[dataset_data['Test Score'].idxmax(), 'Framework']\n",
    "                best_score_for_dataset = dataset_data['Test Score'].max()\n",
    "                print(f\"  ‚Ä¢ {dataset}: Best = {best_framework_for_dataset} ({best_score_for_dataset:.4f})\")\n",
    "        \n",
    "        # Framework success rates\n",
    "        print(f\"\\nüìà Framework Success Rates:\")\n",
    "        for framework in available_frameworks_full:\n",
    "            framework_results = df_results[df_results['Framework'] == framework]\n",
    "            success_rate = len(framework_results) / len(FULL_EXP_CONFIG['datasets']) * 100\n",
    "            avg_score = framework_results['Test Score'].mean() if len(framework_results) > 0 else 0\n",
    "            print(f\"  ‚Ä¢ {framework}: {success_rate:.1f}% success rate, avg score: {avg_score:.4f}\")\n",
    "        \n",
    "        # Kolosal-specific analysis\n",
    "        if KOLOSAL_AVAILABLE and 'Kolosal AutoML' in df_results['Framework'].values:\n",
    "            print(f\"\\nüèóÔ∏è Kolosal AutoML Performance Analysis:\")\n",
    "            kolosal_data = df_results[df_results['Framework'] == 'Kolosal AutoML']\n",
    "            \n",
    "            print(f\"  ‚Ä¢ Successful runs: {len(kolosal_data)}/{len(FULL_EXP_CONFIG['datasets'])}\")\n",
    "            print(f\"  ‚Ä¢ Average accuracy: {kolosal_data['Test Score'].mean():.4f} ¬± {kolosal_data['Test Score'].std():.4f}\")\n",
    "            print(f\"  ‚Ä¢ Average training time: {kolosal_data['Training Time (s)'].mean():.2f}s\")\n",
    "            print(f\"  ‚Ä¢ Average memory usage: {kolosal_data['Memory Peak (MB)'].mean():.1f}MB\")\n",
    "            \n",
    "            # Ranking analysis\n",
    "            framework_rankings = df_results.groupby('Framework')['Test Score'].mean().sort_values(ascending=False)\n",
    "            kolosal_rank = list(framework_rankings.index).index('Kolosal AutoML') + 1 if 'Kolosal AutoML' in framework_rankings.index else \"N/A\"\n",
    "            print(f\"  ‚Ä¢ Overall ranking: #{kolosal_rank} out of {len(framework_rankings)} frameworks\")\n",
    "            \n",
    "            # Dataset-specific performance\n",
    "            print(f\"  ‚Ä¢ Dataset performance:\")\n",
    "            for dataset in kolosal_data['Dataset'].unique():\n",
    "                dataset_kolosal = kolosal_data[kolosal_data['Dataset'] == dataset]\n",
    "                dataset_all = df_results[df_results['Dataset'] == dataset]\n",
    "                kolosal_score = dataset_kolosal['Test Score'].iloc[0]\n",
    "                best_score = dataset_all['Test Score'].max()\n",
    "                relative_performance = (kolosal_score / best_score) * 100 if best_score > 0 else 0\n",
    "                print(f\"    - {dataset}: {kolosal_score:.4f} ({relative_performance:.1f}% of best)\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(f\"\\nüìä Generating comprehensive visualizations...\")\n",
    "    framework_stats = create_comparison_visualizations(df_results)\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\nüìà Performing statistical analysis...\")\n",
    "    perform_statistical_analysis(df_results)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    print(f\"\\nüìÑ Generating final report...\")\n",
    "    generate_comprehensive_report()\n",
    "    \n",
    "    print(f\"\\nüéØ Full Experiment Completed Successfully!\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"üìä Summary:\")\n",
    "    print(f\"  ‚Ä¢ Total benchmark runs: {len(results)}\")\n",
    "    print(f\"  ‚Ä¢ Datasets tested: {len(FULL_EXP_CONFIG['datasets'])}\")\n",
    "    print(f\"  ‚Ä¢ Frameworks tested: {len(available_frameworks_full)}\")\n",
    "    print(f\"  ‚Ä¢ Success rate: {len([r for r in results if r.success]) / len(results) * 100:.1f}%\")\n",
    "    print(f\"  ‚Ä¢ AutoML Performance Mode: {'‚úÖ ENABLED' if FULL_EXP_CONFIG.get('automl_performance_mode', False) else '‚ùå DISABLED'}\")\n",
    "    print(f\"  ‚Ä¢ Optimization Strategy: {FULL_EXP_CONFIG.get('optimization_strategy', 'standard').upper()}\")\n",
    "    print(f\"  ‚Ä¢ Advanced Features: {', '.join([k.replace('_', ' ').title() for k, v in FULL_EXP_CONFIG.items() if k.startswith('enable_') and v])}\")\n",
    "    \n",
    "    if KOLOSAL_AVAILABLE:\n",
    "        print(f\"  ‚Ä¢ Kolosal AutoML included: ‚úÖ (Performance Optimized)\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ Kolosal AutoML available: ‚ùå\")\n",
    "    \n",
    "    print(f\"\\nüí° Next steps:\")\n",
    "    print(f\"  1. Review the visualizations above\")\n",
    "    print(f\"  2. Check the statistical analysis results\")\n",
    "    print(f\"  3. Read the comprehensive report\")\n",
    "    print(f\"  4. Consider running with larger datasets if needed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Experiment failed with error: {str(e)}\")\n",
    "    print(f\"üí° Try reducing the number of datasets or frameworks in FULL_EXP_CONFIG\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nüèÅ Full AutoML Comparison Experiment Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8149f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß AutoML Performance Configuration Verification\n",
    "\n",
    "print(\"üöÄ AutoML Performance Configuration Status\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display current configuration\n",
    "print(f\"üìä Benchmark Configuration:\")\n",
    "print(f\"  ‚Ä¢ Base time budget: {BENCHMARK_CONFIG['time_budget']} seconds\")\n",
    "print(f\"  ‚Ä¢ AutoML time budget: {BENCHMARK_CONFIG.get('automl_time_budget', 'Not set')} seconds\")\n",
    "print(f\"  ‚Ä¢ AutoML optimization: {'‚úÖ ENABLED' if BENCHMARK_CONFIG.get('enable_automl_optimization') else '‚ùå DISABLED'}\")\n",
    "print(f\"  ‚Ä¢ Optimization strategy: {BENCHMARK_CONFIG.get('optimization_strategy', 'Not set').upper()}\")\n",
    "print(f\"  ‚Ä¢ Max workers (AutoML): {BENCHMARK_CONFIG.get('max_workers_automl', 'Not set')}\")\n",
    "print(f\"  ‚Ä¢ Memory optimization: {'‚úÖ ON' if BENCHMARK_CONFIG.get('memory_optimization') else '‚ùå OFF'}\")\n",
    "print(f\"  ‚Ä¢ Early stopping: {'‚úÖ ON' if BENCHMARK_CONFIG.get('enable_early_stopping') else '‚ùå OFF'}\")\n",
    "\n",
    "print(f\"\\nüéØ Full Experiment Configuration:\")\n",
    "print(f\"  ‚Ä¢ Performance mode: {'‚úÖ ENABLED' if FULL_EXP_CONFIG.get('automl_performance_mode') else '‚ùå DISABLED'}\")\n",
    "print(f\"  ‚Ä¢ Advanced optimization: {'‚úÖ ON' if FULL_EXP_CONFIG.get('enable_advanced_optimization') else '‚ùå OFF'}\")\n",
    "print(f\"  ‚Ä¢ Parallel processing: {'‚úÖ ON' if FULL_EXP_CONFIG.get('parallel_processing') else '‚ùå OFF'}\")\n",
    "print(f\"  ‚Ä¢ Ensemble methods: {'‚úÖ ON' if FULL_EXP_CONFIG.get('ensemble_methods') else '‚ùå OFF'}\")\n",
    "print(f\"  ‚Ä¢ Feature selection: {'‚úÖ AUTO' if FULL_EXP_CONFIG.get('feature_selection') else '‚ùå MANUAL'}\")\n",
    "print(f\"  ‚Ä¢ Adaptive hyperopt: {'‚úÖ ON' if FULL_EXP_CONFIG.get('adaptive_hyperopt') else '‚ùå OFF'}\")\n",
    "\n",
    "print(f\"\\nü§ñ Framework-Specific Optimizations:\")\n",
    "if KOLOSAL_AVAILABLE:\n",
    "    print(f\"  ‚Ä¢ Kolosal AutoML: ‚úÖ HYPERX + JIT + Mixed Precision + Streaming\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Kolosal AutoML: ‚ùå Not Available\")\n",
    "\n",
    "if FLAML_AVAILABLE:\n",
    "    print(f\"  ‚Ä¢ FLAML: ‚úÖ Ensemble + Memory Optimization + Early Stopping\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ FLAML: ‚ùå Not Available\")\n",
    "\n",
    "print(f\"  ‚Ä¢ Other frameworks: ‚úÖ Standard optimizations maintained\")\n",
    "\n",
    "print(f\"\\nüìà Performance Features Active:\")\n",
    "performance_features = []\n",
    "if BENCHMARK_CONFIG.get('enable_automl_optimization'):\n",
    "    performance_features.append(\"AutoML Optimization\")\n",
    "if BENCHMARK_CONFIG.get('memory_optimization'):\n",
    "    performance_features.append(\"Memory Management\")\n",
    "if BENCHMARK_CONFIG.get('enable_early_stopping'):\n",
    "    performance_features.append(\"Early Stopping\")\n",
    "if FULL_EXP_CONFIG.get('ensemble_methods'):\n",
    "    performance_features.append(\"Ensemble Methods\")\n",
    "if FULL_EXP_CONFIG.get('feature_selection'):\n",
    "    performance_features.append(\"Auto Feature Selection\")\n",
    "if FULL_EXP_CONFIG.get('adaptive_hyperopt'):\n",
    "    performance_features.append(\"Adaptive Hyperparameter Optimization\")\n",
    "\n",
    "if performance_features:\n",
    "    for i, feature in enumerate(performance_features, 1):\n",
    "        print(f\"  {i}. {feature}\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ No advanced performance features enabled\")\n",
    "\n",
    "print(f\"\\n‚ö° Ready for high-performance AutoML benchmarking!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781e2d1",
   "metadata": {},
   "source": [
    "# üìù Conclusion\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive comparison framework for evaluating different AutoML platforms including:\n",
    "\n",
    "### üèóÔ∏è **Kolosal-AutoML** (Kolosal, Inc)\n",
    "- Our proprietary AutoML system\n",
    "- Optimized for performance and efficiency\n",
    "- Built-in advanced preprocessing and optimization\n",
    "\n",
    "### ü§ñ **Other AutoML Platforms**\n",
    "- **FLAML** (Microsoft): Fast Library for AutoML\n",
    "- **Auto-sklearn**: Automated ML with scikit-learn\n",
    "- **TPOT**: Tree-based Pipeline Optimization Tool\n",
    "- **H2O AutoML**: H2O.ai's AutoML platform\n",
    "- **AutoGluon**: Amazon's AutoML toolkit\n",
    "- **PyCaret**: Low-code ML library\n",
    "- **MLjar-Supervised**: Automated ML for supervised learning\n",
    "\n",
    "### üìä **Evaluation Metrics**\n",
    "- **Performance**: Test accuracy/R¬≤ scores\n",
    "- **Efficiency**: Training time and memory usage\n",
    "- **Consistency**: Cross-validation stability\n",
    "- **Statistical Significance**: ANOVA and t-tests\n",
    "\n",
    "### üîç **Key Features**\n",
    "- Interactive widget-based interface\n",
    "- Real-time progress tracking\n",
    "- Comprehensive visualizations\n",
    "- Statistical analysis and reporting\n",
    "- Support for both classification and regression tasks\n",
    "- Multiple dataset sizes for scalability testing\n",
    "\n",
    "### üí° **Use Cases**\n",
    "1. **Framework Selection**: Choose the best AutoML platform for your needs\n",
    "2. **Performance Benchmarking**: Compare Kolosal-AutoML against competitors\n",
    "3. **Research and Development**: Analyze strengths and weaknesses of different approaches\n",
    "4. **Academic Studies**: Statistical comparison of AutoML methods\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "1. Run the benchmark on your specific datasets\n",
    "2. Adjust time budgets based on your requirements\n",
    "3. Analyze results for your specific use case\n",
    "4. Consider ensemble methods combining multiple frameworks\n",
    "\n",
    "---\n",
    "\n",
    "**Happy benchmarking! üéØ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
